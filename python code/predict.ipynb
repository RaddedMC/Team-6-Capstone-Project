{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37eed681-a747-4573-ab28-be7bcc39c5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\pytorchMethod\\pytorchClassification2\\pytorchEnv2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: A_test.jpg result A\n",
      "2: B_test.jpg result B\n",
      "3: C_test.jpg result C\n",
      "4: D_test.jpg result D\n",
      "5: E_test.jpg result E\n",
      "6: F_test.jpg result F\n",
      "7: G_test.jpg result G\n",
      "8: H_test.jpg result H\n",
      "9: I_test.jpg result I\n",
      "10: J_test.jpg result J\n",
      "11: K_test.jpg result K\n",
      "12: L_test.jpg result L\n",
      "13: M_test.jpg result M\n",
      "14: nothing_test.jpg result nothing\n",
      "15: N_test.jpg result N\n",
      "16: O_test.jpg result O\n",
      "17: P_test.jpg result P\n",
      "18: Q_test.jpg result Q\n",
      "19: R_test.jpg result R\n",
      "20: space_test.jpg result space\n",
      "21: S_test.jpg result S\n",
      "22: T_test.jpg result T\n",
      "23: U_test.jpg result U\n",
      "24: V_test.jpg result V\n",
      "25: W_test.jpg result W\n",
      "26: X_test.jpg result X\n",
      "27: Y_test.jpg result Y\n",
      "28: Z_test.jpg result Z\n",
      "1: aa.jpg result S\n",
      "2: b.jpg result B\n",
      "3: bb.jpg result B\n",
      "4: bbbbbb.jpg result F\n",
      "5: cc.jpg result C\n",
      "6: cccc.jpg result F\n",
      "7: dd.jpg result space\n",
      "8: dddd.jpg result F\n",
      "9: ee.jpg result S\n",
      "10: ff.jpg result F\n",
      "11: gg.jpg result G\n",
      "12: hh.jpg result H\n",
      "13: jj.jpg result J\n",
      "14: kk.jpg result K\n",
      "15: ll.jpg result I\n",
      "16: llll.jpg result L\n",
      "17: mmm.jpg result S\n",
      "18: nothing.jpg result nothing\n",
      "19: ooo.jpg result O\n",
      "20: pppp.jpg result P\n",
      "21: qqqq.jpg result Q\n",
      "22: rrr.jpg result R\n",
      "23: rrrrrr.jpg result X\n",
      "24: space.jpg result space\n",
      "25: ssss.jpg result S\n",
      "26: sssss.jpg result N\n",
      "27: tt.jpg result J\n",
      "28: uu.jpg result U\n",
      "29: vv.jpg result K\n",
      "30: www.jpg result W\n",
      "31: xxx.jpg result X\n",
      "32: yyy.jpg result Y\n",
      "33: zzzz.jpg result X\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# from train_resnet import SelfNet\n",
    "import os\n",
    "import os.path as osp\n",
    "import shutil\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import timm\n",
    "from torchvision import transforms\n",
    "\n",
    "#set to gpu\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "model_path = \"./savefile/efficientnet_b3a_pretrained_224_111/efficientnet_b3a_10epochs_accuracy1.00000_weights.pth\"\n",
    "#'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space'\n",
    "classes_names = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space'] \n",
    "img_size = 224 \n",
    "model_name = \"efficientnet_b3a\"\n",
    "num_classes = len(classes_names)  \n",
    "\n",
    "\n",
    "#---------\n",
    "\n",
    "# define the model\n",
    "class SELFMODEL(nn.Module):\n",
    "    def __init__(self, model_name, out_features=num_classes,\n",
    "                 pretrained=True):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained)  # load pretrained model\n",
    "        # classifier\n",
    "        if model_name[:3] == \"res\":\n",
    "            n_features = self.model.fc.in_features  # fix the connect layer number\n",
    "            self.model.fc = nn.Linear(n_features, out_features)  # change the class numebr\n",
    "        elif model_name[:3] == \"vit\":\n",
    "            n_features = self.model.head.in_features  # change full connect layer number\n",
    "            self.model.head = nn.Linear(n_features, out_features)\n",
    "        else:\n",
    "            n_features = self.model.classifier.in_features\n",
    "            self.model.classifier = nn.Linear(n_features, out_features)\n",
    "        # resnet  change the all connect layer\n",
    "   #     print(self.model)  # return model\n",
    "\n",
    "    def forward(self, x):  # front spreed\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "def get_torch_transforms(img_size=224):\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            # transforms.RandomResizedCrop(img_size),\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.RandomHorizontalFlip(p=0.2),\n",
    "            transforms.RandomRotation((-5, 5)),\n",
    "            transforms.RandomAutocontrast(p=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            # transforms.Resize((img_size, img_size)),\n",
    "            # transforms.Resize(256),\n",
    "            # transforms.CenterCrop(img_size),\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "        # Convert input image to RGB format\n",
    "    #data_transforms['val'].transforms.insert(0, transforms.Grayscale(3))\n",
    "    return data_transforms\n",
    "    \n",
    "#---------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_batch(model_path, target_dir, save_dir):\n",
    "    data_transforms = get_torch_transforms(img_size=img_size)\n",
    "    valid_transforms = data_transforms['val']\n",
    "    # load the net\n",
    "    model = SELFMODEL(model_name=model_name, out_features=num_classes, pretrained=False)\n",
    "    # model = nn.DataParallel(model)\n",
    "    weights = torch.load(model_path)\n",
    "    model.load_state_dict(weights)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    # read picture\n",
    "    image_names = os.listdir(target_dir)\n",
    "    for i, image_name in enumerate(image_names):\n",
    "        image_path = osp.join(target_dir, image_name)\n",
    "        img = Image.open(image_path)\n",
    "        img = valid_transforms(img)\n",
    "        img = img.unsqueeze(0)\n",
    "        img = img.to(device)\n",
    "        output = model(img)\n",
    "        label_id = torch.argmax(output).item()\n",
    "        predict_name = classes_names[label_id]\n",
    "        save_path = osp.join(save_dir, predict_name)\n",
    "        if not osp.isdir(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        shutil.copy(image_path, save_path)\n",
    "        print(f\"{i + 1}: {image_name} result {predict_name}\")\n",
    "\n",
    "\n",
    "def predict_single(model_path, image_path):\n",
    "    data_transforms = get_torch_transforms(img_size=img_size)\n",
    "    # train_transforms = data_transforms['train']\n",
    "    valid_transforms = data_transforms['val']\n",
    "    # load the net\n",
    "    model = SELFMODEL(model_name=model_name, out_features=num_classes, pretrained=False)\n",
    "    # model = nn.DataParallel(model)\n",
    "    weights = torch.load(model_path)\n",
    "    model.load_state_dict(weights)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # read picture\n",
    "    img = Image.open(image_path)\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    img = valid_transforms(img)\n",
    "    img = img.unsqueeze(0)\n",
    "    img = img.to(device)\n",
    "    output = model(img)\n",
    "    label_id = torch.argmax(output).item()\n",
    "    predict_name = classes_names[label_id]\n",
    "    print(f\"{image_path}'s result is {predict_name}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # bacth predict\n",
    "    \n",
    "    #target_dir=\"./gesture_finalpredict/asl_alphabet_test\"\n",
    "   # predict_batch(model_path=model_path,target_dir=\"./americanSignGesture/professorhand/gesture_finalpredict/test2\",save_dir=\"./americanSignGesture/professorhand/gesture_finalpredict/theresultfortest2\")\n",
    "    \n",
    "  #  predict_batch(model_path=model_path,target_dir=\"./gesture_finalpredict/asl_alphabet_test\",save_dir=\"./gesture_finalpredict/theresultfortest2\")\n",
    "    \n",
    "    # single predict\n",
    "    #predict_single(model_path=model_path, image_path=\"123.jpg\")\n",
    "    predict_batch(model_path=model_path,target_dir=\"./gesture_finalpredict/asl_alphabet_test\",save_dir=\"./gesture_finalpredict/theresultfortest2\")\n",
    "    predict_batch(model_path=model_path,target_dir=\"./gesture_finalpredict/test2\",save_dir=\"./gesture_finalpredict/theresultfortest2\")\n",
    "    #predict_single(model_path=model_path, image_path=\"2.jpg\")\n",
    "    \n",
    "   # predict_single(model_path=model_path, image_path=\"3.jpg\")\n",
    "    \n",
    "  #  predict_single(model_path=model_path, image_path=\"4.jpg\")\n",
    "    \n",
    "  #  predict_single(model_path=model_path, image_path=\"5.jpg\")\n",
    "    \n",
    " #   predict_single(model_path=model_path, image_path=\"6.jpg\")\n",
    "    \n",
    "  #  predict_single(model_path=model_path, image_path=\"7.jpg\")\n",
    "    \n",
    "   #  predict_single(model_path=model_path, image_path=\"./gesture_finalpredict/test2/b.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1ca71b7-8075-4061-9cd9-a02d87e37fdc",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SELFMODEL:\n\tsize mismatch for model.classifier.weight: copying a param with shape torch.Size([29, 1536]) from checkpoint, the shape in current model is torch.Size([27, 1536]).\n\tsize mismatch for model.classifier.bias: copying a param with shape torch.Size([29]) from checkpoint, the shape in current model is torch.Size([27]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 138\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms result is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredict_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m# bacth predict\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \n\u001b[0;32m    135\u001b[0m     \u001b[38;5;66;03m# target_dir=\"./gesture_finalpredict/asl_alphabet_test\"\u001b[39;00m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;66;03m# predict_batch(model_path=model_path,target_dir=\"./americanSignGesture/professorhand/gesture_finalpredict/test2\",save_dir=\"./americanSignGesture/professorhand/gesture_finalpredict/theresultfortest2\")\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m     \u001b[43mpredict_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtarget_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./gesture_finalpredict/test2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./gesture_finalpredict/theresultfortest2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;66;03m# single predict\u001b[39;00m\n\u001b[0;32m    141\u001b[0m     predict_single(model_path\u001b[38;5;241m=\u001b[39mmodel_path, image_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m123.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 86\u001b[0m, in \u001b[0;36mpredict_batch\u001b[1;34m(model_path, target_dir, save_dir)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# model = nn.DataParallel(model)\u001b[39;00m\n\u001b[0;32m     85\u001b[0m weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(model_path)\n\u001b[1;32m---> 86\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     88\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mD:\\pytorchMethod\\pytorchClassification2\\pytorchEnv2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SELFMODEL:\n\tsize mismatch for model.classifier.weight: copying a param with shape torch.Size([29, 1536]) from checkpoint, the shape in current model is torch.Size([27, 1536]).\n\tsize mismatch for model.classifier.bias: copying a param with shape torch.Size([29]) from checkpoint, the shape in current model is torch.Size([27])."
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "# from train_resnet import SelfNet\n",
    "import os\n",
    "import os.path as osp\n",
    "import shutil\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import timm\n",
    "from torchvision import transforms\n",
    "\n",
    "# set to gpu\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "model_path = \"./savefile/efficientnet_b3a_pretrained_224/efficientnet_b3a_10epochs_accuracy1.00000_weights.pth\"\n",
    "# 'A', 'B', 'Blank', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'\n",
    "# classes_names = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n",
    "\n",
    "classes_names = ['A', 'B', 'Blank', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "img_size = 224\n",
    "model_name = \"efficientnet_b3a\"\n",
    "num_classes = len(classes_names)\n",
    "\n",
    "# ---------\n",
    "\n",
    "# define the model\n",
    "class SELFMODEL(nn.Module):\n",
    "    def __init__(self, model_name, out_features=num_classes,\n",
    "                 pretrained=True):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained)  # load pretrained model\n",
    "        # classifier\n",
    "        if model_name[:3] == \"res\":\n",
    "            n_features = self.model.fc.in_features  # fix the connect layer number\n",
    "            self.model.fc = nn.Linear(n_features, out_features)  # change the class numebr\n",
    "        elif model_name[:3] == \"vit\":\n",
    "            n_features = self.model.head.in_features  # change full connect layer number\n",
    "            self.model.head = nn.Linear(n_features, out_features)\n",
    "        else:\n",
    "            n_features = self.model.classifier.in_features\n",
    "            self.model.classifier = nn.Linear(n_features, out_features)\n",
    "        # resnet  change the all connect layer\n",
    "    #     print(self.model)  # return model\n",
    "\n",
    "    def forward(self, x):  # front spreed\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def get_torch_transforms(img_size=224):\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            # transforms.RandomResizedCrop(img_size),\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.RandomHorizontalFlip(p=0.2),\n",
    "            transforms.RandomRotation((-5, 5)),\n",
    "            transforms.RandomAutocontrast(p=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            # transforms.Resize((img_size, img_size)),\n",
    "            # transforms.Resize(256),\n",
    "            # transforms.CenterCrop(img_size),\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "    # Convert input image to RGB format\n",
    "    # data_transforms['val'].transforms.insert(0, transforms.Grayscale(3))\n",
    "    return data_transforms\n",
    "\n",
    "# ---------\n",
    "\n",
    "\n",
    "def predict_batch(model_path, target_dir, save_dir):\n",
    "    data_transforms = get_torch_transforms(img_size=img_size)\n",
    "    valid_transforms = data_transforms['val']\n",
    "    # load the net\n",
    "    model = SELFMODEL(model_name=model_name, out_features=num_classes, pretrained=False)\n",
    "    # model = nn.DataParallel(model)\n",
    "    weights = torch.load(model_path)\n",
    "    model.load_state_dict(weights)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    # read picture\n",
    "    image_names = os.listdir(target_dir)\n",
    "    for i, image_name in enumerate(image_names):\n",
    "        image_path = osp.join(target_dir, image_name)\n",
    "        img = Image.open(image_path)\n",
    "        img = valid_transforms(img)\n",
    "        img = img.unsqueeze(0)\n",
    "        img = img.to(device)\n",
    "        output = model(img)\n",
    "        label_id = torch.argmax(output).item()\n",
    "        predict_name = classes_names[label_id]\n",
    "        save_path = osp.join(save_dir, predict_name)\n",
    "        if not osp.isdir(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        shutil.copy(image_path, save_path)\n",
    "        print(f\"{i + 1}: {image_name} result {predict_name}\")\n",
    "\n",
    "\n",
    "def predict_single(model_path, image_path):\n",
    "    data_transforms = get_torch_transforms(img_size=img_size)\n",
    "    # train_transforms = data_transforms['train']\n",
    "    valid_transforms = data_transforms['val']\n",
    "    # load the net\n",
    "    model = SELFMODEL(model_name=model_name, out_features=num_classes, pretrained=False)\n",
    "    # model = nn.DataParallel(model)\n",
    "    weights = torch.load(model_path)\n",
    "    model.load_state_dict(weights)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # read picture\n",
    "    img = Image.open(image_path)\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    img = valid_transforms(img)\n",
    "    img = img.unsqueeze(0)\n",
    "    img = img.to(device)\n",
    "    output = model(img)\n",
    "    label_id = torch.argmax(output).item()\n",
    "    predict_name = classes_names[label_id]\n",
    "    print(f\"{image_path}'s result is {predict_name}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # bacth predict\n",
    "\n",
    "    # target_dir=\"./gesture_finalpredict/asl_alphabet_test\"\n",
    "    # predict_batch(model_path=model_path,target_dir=\"./americanSignGesture/professorhand/gesture_finalpredict/test2\",save_dir=\"./americanSignGesture/professorhand/gesture_finalpredict/theresultfortest2\")\n",
    "\n",
    "    predict_batch(model_path=model_path,target_dir=\"./gesture_finalpredict/test2\",save_dir=\"./gesture_finalpredict/theresultfortest2\")\n",
    "\n",
    "    # single predict\n",
    "    predict_single(model_path=model_path, image_path=\"123.jpg\")\n",
    "\n",
    "    # predict_single(model_path=model_path, image_path=\"2.jpg\")\n",
    "\n",
    "# predict_single(model_path=model_path, image_path=\"3.jpg\")\n",
    "\n",
    "#  predict_single(model_path=model_path, image_path=\"4.jpg\")\n",
    "\n",
    "#  predict_single(model_path=model_path, image_path=\"5.jpg\")\n",
    "\n",
    "#   predict_single(model_path=model_path, image_path=\"6.jpg\")\n",
    "\n",
    "#  predict_single(model_path=model_path, image_path=\"7.jpg\")\n",
    "\n",
    "#  predict_single(model_path=model_path, image_path=\"./gesture_finalpredict/test2/b.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7736ccf4-ef48-4246-985a-9a901fb3f961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: A_test.jpg result A\n",
      "2: B_test.jpg result B\n",
      "3: C_test.jpg result C\n",
      "4: D_test.jpg result D\n",
      "5: E_test.jpg result E\n",
      "6: F_test.jpg result F\n",
      "7: G_test.jpg result G\n",
      "8: H_test.jpg result H\n",
      "9: I_test.jpg result I\n",
      "10: J_test.jpg result J\n",
      "11: K_test.jpg result K\n",
      "12: L_test.jpg result L\n",
      "13: M_test.jpg result M\n",
      "14: nothing_test.jpg result nothing\n",
      "15: N_test.jpg result N\n",
      "16: O_test.jpg result O\n",
      "17: P_test.jpg result P\n",
      "18: Q_test.jpg result Q\n",
      "19: R_test.jpg result R\n",
      "20: space_test.jpg result space\n",
      "21: S_test.jpg result S\n",
      "22: T_test.jpg result T\n",
      "23: U_test.jpg result U\n",
      "24: V_test.jpg result V\n",
      "25: W_test.jpg result W\n",
      "26: X_test.jpg result X\n",
      "27: Y_test.jpg result Y\n",
      "28: Z_test.jpg result Z\n",
      "1: aa.jpg result S\n",
      "2: b.jpg result B\n",
      "3: bb.jpg result B\n",
      "4: bbbbbb.jpg result B\n",
      "5: cc.jpg result C\n",
      "6: cccc.jpg result P\n",
      "7: dd.jpg result D\n",
      "8: dddd.jpg result D\n",
      "9: ee.jpg result S\n",
      "10: ff.jpg result F\n",
      "11: gg.jpg result G\n",
      "12: hh.jpg result H\n",
      "13: jj.jpg result J\n",
      "14: kk.jpg result K\n",
      "15: ll.jpg result I\n",
      "16: llll.jpg result L\n",
      "17: mmm.jpg result N\n",
      "18: nothing.jpg result nothing\n",
      "19: ooo.jpg result O\n",
      "20: pppp.jpg result P\n",
      "21: qqqq.jpg result Q\n",
      "22: rrr.jpg result H\n",
      "23: rrrrrr.jpg result R\n",
      "24: space.jpg result space\n",
      "25: ssss.jpg result S\n",
      "26: sssss.jpg result J\n",
      "27: tt.jpg result X\n",
      "28: uu.jpg result U\n",
      "29: vv.jpg result K\n",
      "30: www.jpg result W\n",
      "31: xxx.jpg result X\n",
      "32: yyy.jpg result Y\n",
      "33: zzzz.jpg result X\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# from train_resnet import SelfNet\n",
    "import os\n",
    "import os.path as osp\n",
    "import shutil\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import timm\n",
    "from torchvision import transforms\n",
    "\n",
    "#set to gpu\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "model_path = \"./savefile/efficientnet_b3a_pretrained_224/efficientnet_b3a_e10bat42LR5.pth\"\n",
    "#'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space'\n",
    "classes_names = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space'] \n",
    "img_size = 224 \n",
    "model_name = \"efficientnet_b3a\"\n",
    "num_classes = len(classes_names)  \n",
    "\n",
    "\n",
    "#---------\n",
    "\n",
    "# define the model\n",
    "class SELFMODEL(nn.Module):\n",
    "    def __init__(self, model_name, out_features=num_classes,\n",
    "                 pretrained=True):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained)  # load pretrained model\n",
    "        # classifier\n",
    "        if model_name[:3] == \"res\":\n",
    "            n_features = self.model.fc.in_features  # fix the connect layer number\n",
    "            self.model.fc = nn.Linear(n_features, out_features)  # change the class numebr\n",
    "        elif model_name[:3] == \"vit\":\n",
    "            n_features = self.model.head.in_features  # change full connect layer number\n",
    "            self.model.head = nn.Linear(n_features, out_features)\n",
    "        else:\n",
    "            n_features = self.model.classifier.in_features\n",
    "            self.model.classifier = nn.Linear(n_features, out_features)\n",
    "        # resnet  change the all connect layer\n",
    "   #     print(self.model)  # return model\n",
    "\n",
    "    def forward(self, x):  # front spreed\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "def get_torch_transforms(img_size=224):\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            # transforms.RandomResizedCrop(img_size),\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.RandomHorizontalFlip(p=0.2),\n",
    "            transforms.RandomRotation((-5, 5)),\n",
    "            transforms.RandomAutocontrast(p=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            # transforms.Resize((img_size, img_size)),\n",
    "            # transforms.Resize(256),\n",
    "            # transforms.CenterCrop(img_size),\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "        # Convert input image to RGB format\n",
    "    #data_transforms['val'].transforms.insert(0, transforms.Grayscale(3))\n",
    "    return data_transforms\n",
    "    \n",
    "#---------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_batch(model_path, target_dir, save_dir):\n",
    "    data_transforms = get_torch_transforms(img_size=img_size)\n",
    "    valid_transforms = data_transforms['val']\n",
    "    # load the net\n",
    "    model = SELFMODEL(model_name=model_name, out_features=num_classes, pretrained=False)\n",
    "    # model = nn.DataParallel(model)\n",
    "    weights = torch.load(model_path)\n",
    "    model.load_state_dict(weights)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    # read picture\n",
    "    image_names = os.listdir(target_dir)\n",
    "    for i, image_name in enumerate(image_names):\n",
    "        image_path = osp.join(target_dir, image_name)\n",
    "        img = Image.open(image_path)\n",
    "        img = valid_transforms(img)\n",
    "        img = img.unsqueeze(0)\n",
    "        img = img.to(device)\n",
    "        output = model(img)\n",
    "        label_id = torch.argmax(output).item()\n",
    "        predict_name = classes_names[label_id]\n",
    "        save_path = osp.join(save_dir, predict_name)\n",
    "        if not osp.isdir(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        shutil.copy(image_path, save_path)\n",
    "        print(f\"{i + 1}: {image_name} result {predict_name}\")\n",
    "\n",
    "\n",
    "def predict_single(model_path, image_path):\n",
    "    data_transforms = get_torch_transforms(img_size=img_size)\n",
    "    # train_transforms = data_transforms['train']\n",
    "    valid_transforms = data_transforms['val']\n",
    "    # load the net\n",
    "    model = SELFMODEL(model_name=model_name, out_features=num_classes, pretrained=False)\n",
    "    # model = nn.DataParallel(model)\n",
    "    weights = torch.load(model_path)\n",
    "    model.load_state_dict(weights)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # read picture\n",
    "    img = Image.open(image_path)\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    img = valid_transforms(img)\n",
    "    img = img.unsqueeze(0)\n",
    "    img = img.to(device)\n",
    "    output = model(img)\n",
    "    label_id = torch.argmax(output).item()\n",
    "    predict_name = classes_names[label_id]\n",
    "    print(f\"{image_path}'s result is {predict_name}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # bacth predict\n",
    "\n",
    "    predict_batch(model_path=model_path,target_dir=\"./gesture_finalpredict/asl_alphabet_test\",save_dir=\"./gesture_finalpredict/theresultfortest2\")\n",
    "    predict_batch(model_path=model_path,target_dir=\"./gesture_finalpredict/test2\",save_dir=\"./gesture_finalpredict/theresultfortest2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b832e26f-a55a-4669-8b60-4845f0d212f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: A_test.jpg result A\n",
      "2: B_test.jpg result B\n",
      "3: C_test.jpg result C\n",
      "4: D_test.jpg result D\n",
      "5: E_test.jpg result E\n",
      "6: F_test.jpg result F\n",
      "7: G_test.jpg result G\n",
      "8: H_test.jpg result H\n",
      "9: I_test.jpg result I\n",
      "10: J_test.jpg result J\n",
      "11: K_test.jpg result K\n",
      "12: L_test.jpg result L\n",
      "13: M_test.jpg result M\n",
      "14: nothing_test.jpg result nothing\n",
      "15: N_test.jpg result N\n",
      "16: O_test.jpg result O\n",
      "17: P_test.jpg result P\n",
      "18: Q_test.jpg result Q\n",
      "19: R_test.jpg result R\n",
      "20: space_test.jpg result space\n",
      "21: S_test.jpg result S\n",
      "22: T_test.jpg result T\n",
      "23: U_test.jpg result U\n",
      "24: V_test.jpg result V\n",
      "25: W_test.jpg result W\n",
      "26: X_test.jpg result X\n",
      "27: Y_test.jpg result Y\n",
      "28: Z_test.jpg result Z\n",
      "1: aa.jpg result S\n",
      "2: b.jpg result B\n",
      "3: bb.jpg result B\n",
      "4: bbbbbb.jpg result B\n",
      "5: cc.jpg result C\n",
      "6: cccc.jpg result P\n",
      "7: dd.jpg result D\n",
      "8: dddd.jpg result D\n",
      "9: ee.jpg result S\n",
      "10: ff.jpg result F\n",
      "11: gg.jpg result J\n",
      "12: hh.jpg result H\n",
      "13: jj.jpg result J\n",
      "14: kk.jpg result K\n",
      "15: ll.jpg result I\n",
      "16: llll.jpg result L\n",
      "17: mmm.jpg result N\n",
      "18: nothing.jpg result nothing\n",
      "19: ooo.jpg result O\n",
      "20: pppp.jpg result P\n",
      "21: qqqq.jpg result Q\n",
      "22: rrr.jpg result H\n",
      "23: rrrrrr.jpg result R\n",
      "24: space.jpg result space\n",
      "25: ssss.jpg result S\n",
      "26: sssss.jpg result J\n",
      "27: tt.jpg result X\n",
      "28: uu.jpg result U\n",
      "29: vv.jpg result K\n",
      "30: www.jpg result W\n",
      "31: xxx.jpg result X\n",
      "32: yyy.jpg result Y\n",
      "33: zzzz.jpg result X\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# from train_resnet import SelfNet\n",
    "import os\n",
    "import os.path as osp\n",
    "import shutil\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import timm\n",
    "from torchvision import transforms\n",
    "\n",
    "#set to gpu\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "model_path = \"./savefile/efficientnet_b3a_pretrained_224/efficientnet_b3a_20epochsLR5Batch36.pth\"\n",
    "#'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space'\n",
    "classes_names = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space'] \n",
    "img_size = 224 \n",
    "model_name = \"efficientnet_b3a\"\n",
    "num_classes = len(classes_names)  \n",
    "\n",
    "\n",
    "#---------\n",
    "\n",
    "# define the model\n",
    "class SELFMODEL(nn.Module):\n",
    "    def __init__(self, model_name, out_features=num_classes,\n",
    "                 pretrained=True):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained)  # load pretrained model\n",
    "        # classifier\n",
    "        if model_name[:3] == \"res\":\n",
    "            n_features = self.model.fc.in_features  # fix the connect layer number\n",
    "            self.model.fc = nn.Linear(n_features, out_features)  # change the class numebr\n",
    "        elif model_name[:3] == \"vit\":\n",
    "            n_features = self.model.head.in_features  # change full connect layer number\n",
    "            self.model.head = nn.Linear(n_features, out_features)\n",
    "        else:\n",
    "            n_features = self.model.classifier.in_features\n",
    "            self.model.classifier = nn.Linear(n_features, out_features)\n",
    "        # resnet  change the all connect layer\n",
    "   #     print(self.model)  # return model\n",
    "\n",
    "    def forward(self, x):  # front spreed\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "def get_torch_transforms(img_size=224):\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            # transforms.RandomResizedCrop(img_size),\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.RandomHorizontalFlip(p=0.2),\n",
    "            transforms.RandomRotation((-5, 5)),\n",
    "            transforms.RandomAutocontrast(p=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            # transforms.Resize((img_size, img_size)),\n",
    "            # transforms.Resize(256),\n",
    "            # transforms.CenterCrop(img_size),\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "        # Convert input image to RGB format\n",
    "    #data_transforms['val'].transforms.insert(0, transforms.Grayscale(3))\n",
    "    return data_transforms\n",
    "    \n",
    "#---------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_batch(model_path, target_dir, save_dir):\n",
    "    data_transforms = get_torch_transforms(img_size=img_size)\n",
    "    valid_transforms = data_transforms['val']\n",
    "    # load the net\n",
    "    model = SELFMODEL(model_name=model_name, out_features=num_classes, pretrained=False)\n",
    "    # model = nn.DataParallel(model)\n",
    "    weights = torch.load(model_path)\n",
    "    model.load_state_dict(weights)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    # read picture\n",
    "    image_names = os.listdir(target_dir)\n",
    "    for i, image_name in enumerate(image_names):\n",
    "        image_path = osp.join(target_dir, image_name)\n",
    "        img = Image.open(image_path)\n",
    "        img = valid_transforms(img)\n",
    "        img = img.unsqueeze(0)\n",
    "        img = img.to(device)\n",
    "        output = model(img)\n",
    "        label_id = torch.argmax(output).item()\n",
    "        predict_name = classes_names[label_id]\n",
    "        save_path = osp.join(save_dir, predict_name)\n",
    "        if not osp.isdir(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        shutil.copy(image_path, save_path)\n",
    "        print(f\"{i + 1}: {image_name} result {predict_name}\")\n",
    "\n",
    "\n",
    "def predict_single(model_path, image_path):\n",
    "    data_transforms = get_torch_transforms(img_size=img_size)\n",
    "    # train_transforms = data_transforms['train']\n",
    "    valid_transforms = data_transforms['val']\n",
    "    # load the net\n",
    "    model = SELFMODEL(model_name=model_name, out_features=num_classes, pretrained=False)\n",
    "    # model = nn.DataParallel(model)\n",
    "    weights = torch.load(model_path)\n",
    "    model.load_state_dict(weights)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # read picture\n",
    "    img = Image.open(image_path)\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    img = valid_transforms(img)\n",
    "    img = img.unsqueeze(0)\n",
    "    img = img.to(device)\n",
    "    output = model(img)\n",
    "    label_id = torch.argmax(output).item()\n",
    "    predict_name = classes_names[label_id]\n",
    "    print(f\"{image_path}'s result is {predict_name}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # bacth predict\n",
    "\n",
    "    predict_batch(model_path=model_path,target_dir=\"./gesture_finalpredict/asl_alphabet_test\",save_dir=\"./gesture_finalpredict/theresultfortest2\")\n",
    "    predict_batch(model_path=model_path,target_dir=\"./gesture_finalpredict/test2\",save_dir=\"./gesture_finalpredict/theresultfortest2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e084e5f3-afeb-4555-b7f8-356e0d518069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: A_test.jpg result A\n",
      "2: B_test.jpg result B\n",
      "3: C_test.jpg result C\n",
      "4: D_test.jpg result D\n",
      "5: E_test.jpg result E\n",
      "6: F_test.jpg result F\n",
      "7: G_test.jpg result G\n",
      "8: H_test.jpg result H\n",
      "9: I_test.jpg result I\n",
      "10: J_test.jpg result J\n",
      "11: K_test.jpg result K\n",
      "12: L_test.jpg result L\n",
      "13: M_test.jpg result M\n",
      "14: nothing_test.jpg result nothing\n",
      "15: N_test.jpg result N\n",
      "16: O_test.jpg result O\n",
      "17: P_test.jpg result P\n",
      "18: Q_test.jpg result Q\n",
      "19: R_test.jpg result R\n",
      "20: space_test.jpg result space\n",
      "21: S_test.jpg result S\n",
      "22: T_test.jpg result T\n",
      "23: U_test.jpg result U\n",
      "24: V_test.jpg result V\n",
      "25: W_test.jpg result W\n",
      "26: X_test.jpg result X\n",
      "27: Y_test.jpg result Y\n",
      "28: Z_test.jpg result Z\n",
      "1: aa.jpg result S\n",
      "2: b.jpg result B\n",
      "3: bb.jpg result N\n",
      "4: bbbbbb.jpg result B\n",
      "5: cc.jpg result C\n",
      "6: cccc.jpg result Q\n",
      "7: dd.jpg result D\n",
      "8: dddd.jpg result D\n",
      "9: ee.jpg result J\n",
      "10: ff.jpg result N\n",
      "11: gg.jpg result J\n",
      "12: hh.jpg result H\n",
      "13: jj.jpg result J\n",
      "14: kk.jpg result K\n",
      "15: ll.jpg result I\n",
      "16: llll.jpg result L\n",
      "17: mmm.jpg result S\n",
      "18: nothing.jpg result nothing\n",
      "19: ooo.jpg result P\n",
      "20: pppp.jpg result P\n",
      "21: qqqq.jpg result Q\n",
      "22: rrr.jpg result H\n",
      "23: rrrrrr.jpg result R\n",
      "24: space.jpg result space\n",
      "25: ssss.jpg result J\n",
      "26: sssss.jpg result A\n",
      "27: tt.jpg result J\n",
      "28: uu.jpg result U\n",
      "29: vv.jpg result K\n",
      "30: www.jpg result W\n",
      "31: xxx.jpg result X\n",
      "32: yyy.jpg result Y\n",
      "33: zzzz.jpg result J\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# from train_resnet import SelfNet\n",
    "import os\n",
    "import os.path as osp\n",
    "import shutil\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import timm\n",
    "from torchvision import transforms\n",
    "\n",
    "#set to gpu\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "model_path = \"./savefile/efficientnet_b3a_pretrained_224/efficientnet_b3a_20epochsbatch36LR4.pth\"\n",
    "#'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space'\n",
    "classes_names = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space'] \n",
    "img_size = 224 \n",
    "model_name = \"efficientnet_b3a\"\n",
    "num_classes = len(classes_names)  \n",
    "\n",
    "\n",
    "#---------\n",
    "\n",
    "# define the model\n",
    "class SELFMODEL(nn.Module):\n",
    "    def __init__(self, model_name, out_features=num_classes,\n",
    "                 pretrained=True):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained)  # load pretrained model\n",
    "        # classifier\n",
    "        if model_name[:3] == \"res\":\n",
    "            n_features = self.model.fc.in_features  # fix the connect layer number\n",
    "            self.model.fc = nn.Linear(n_features, out_features)  # change the class numebr\n",
    "        elif model_name[:3] == \"vit\":\n",
    "            n_features = self.model.head.in_features  # change full connect layer number\n",
    "            self.model.head = nn.Linear(n_features, out_features)\n",
    "        else:\n",
    "            n_features = self.model.classifier.in_features\n",
    "            self.model.classifier = nn.Linear(n_features, out_features)\n",
    "        # resnet  change the all connect layer\n",
    "   #     print(self.model)  # return model\n",
    "\n",
    "    def forward(self, x):  # front spreed\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "def get_torch_transforms(img_size=224):\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            # transforms.RandomResizedCrop(img_size),\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.RandomHorizontalFlip(p=0.2),\n",
    "            transforms.RandomRotation((-5, 5)),\n",
    "            transforms.RandomAutocontrast(p=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            # transforms.Resize((img_size, img_size)),\n",
    "            # transforms.Resize(256),\n",
    "            # transforms.CenterCrop(img_size),\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "        # Convert input image to RGB format\n",
    "    #data_transforms['val'].transforms.insert(0, transforms.Grayscale(3))\n",
    "    return data_transforms\n",
    "    \n",
    "#---------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_batch(model_path, target_dir, save_dir):\n",
    "    data_transforms = get_torch_transforms(img_size=img_size)\n",
    "    valid_transforms = data_transforms['val']\n",
    "    # load the net\n",
    "    model = SELFMODEL(model_name=model_name, out_features=num_classes, pretrained=False)\n",
    "    # model = nn.DataParallel(model)\n",
    "    weights = torch.load(model_path)\n",
    "    model.load_state_dict(weights)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    # read picture\n",
    "    image_names = os.listdir(target_dir)\n",
    "    for i, image_name in enumerate(image_names):\n",
    "        image_path = osp.join(target_dir, image_name)\n",
    "        img = Image.open(image_path)\n",
    "        img = valid_transforms(img)\n",
    "        img = img.unsqueeze(0)\n",
    "        img = img.to(device)\n",
    "        output = model(img)\n",
    "        label_id = torch.argmax(output).item()\n",
    "        predict_name = classes_names[label_id]\n",
    "        save_path = osp.join(save_dir, predict_name)\n",
    "        if not osp.isdir(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        shutil.copy(image_path, save_path)\n",
    "        print(f\"{i + 1}: {image_name} result {predict_name}\")\n",
    "\n",
    "\n",
    "def predict_single(model_path, image_path):\n",
    "    data_transforms = get_torch_transforms(img_size=img_size)\n",
    "    # train_transforms = data_transforms['train']\n",
    "    valid_transforms = data_transforms['val']\n",
    "    # load the net\n",
    "    model = SELFMODEL(model_name=model_name, out_features=num_classes, pretrained=False)\n",
    "    # model = nn.DataParallel(model)\n",
    "    weights = torch.load(model_path)\n",
    "    model.load_state_dict(weights)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # read picture\n",
    "    img = Image.open(image_path)\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    img = valid_transforms(img)\n",
    "    img = img.unsqueeze(0)\n",
    "    img = img.to(device)\n",
    "    output = model(img)\n",
    "    label_id = torch.argmax(output).item()\n",
    "    predict_name = classes_names[label_id]\n",
    "    print(f\"{image_path}'s result is {predict_name}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # bacth predict\n",
    "\n",
    "    predict_batch(model_path=model_path,target_dir=\"./gesture_finalpredict/asl_alphabet_test\",save_dir=\"./gesture_finalpredict/theresultfortest2\")\n",
    "    predict_batch(model_path=model_path,target_dir=\"./gesture_finalpredict/test2\",save_dir=\"./gesture_finalpredict/theresultfortest2\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
