{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31d9ae05-0220-446a-9f91-0fbd5abaa814",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unreal\n",
      "  Using cached unreal-0.1.1-py3-none-any.whl (2.4 kB)\n",
      "Installing collected packages: unreal\n",
      "Successfully installed unreal-0.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install unreal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d43597fd-e441-4c21-b91a-036fd73d8f4a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in d:\\pytorchmethod\\pytorchclassification2\\pytorchenv2\\lib\\site-packages (23.2.1)\n",
      "Collecting pip\n",
      "  Obtaining dependency information for pip from https://files.pythonhosted.org/packages/15/aa/3f4c7bcee2057a76562a5b33ecbd199be08cdb4443a02e26bd2c3cf6fc39/pip-23.3.2-py3-none-any.whl.metadata\n",
      "  Downloading pip-23.3.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Downloading pip-23.3.2-py3-none-any.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/2.1 MB 4.5 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 0.2/2.1 MB 3.4 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 0.4/2.1 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.6/2.1 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.7/2.1 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.9/2.1 MB 3.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.1/2.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.4/2.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.6/2.1 MB 4.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.9/2.1 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.1/2.1 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 4.2 MB/s eta 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: To modify pip, please run the following command:\n",
      "D:\\pytorchMethod\\pytorchClassification2\\pytorchEnv2\\Scripts\\python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77e3b39f-5fcd-4de5-8097-b18809a08393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyautogui\n",
      "  Using cached PyAutoGUI-0.9.54-py3-none-any.whl\n",
      "Collecting pymsgbox (from pyautogui)\n",
      "  Using cached PyMsgBox-1.0.9-py3-none-any.whl\n",
      "Collecting pytweening>=1.0.4 (from pyautogui)\n",
      "  Using cached pytweening-1.0.7-py3-none-any.whl\n",
      "Collecting pyscreeze>=0.1.21 (from pyautogui)\n",
      "  Using cached PyScreeze-0.1.30-py3-none-any.whl\n",
      "Collecting pygetwindow>=0.0.5 (from pyautogui)\n",
      "  Using cached PyGetWindow-0.0.9-py3-none-any.whl\n",
      "Collecting mouseinfo (from pyautogui)\n",
      "  Using cached MouseInfo-0.1.3-py3-none-any.whl\n",
      "Collecting pyrect (from pygetwindow>=0.0.5->pyautogui)\n",
      "  Using cached PyRect-0.2.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: Pillow>=9.3.0 in d:\\pytorchmethod\\pytorchclassification2\\pytorchenv2\\lib\\site-packages (from pyscreeze>=0.1.21->pyautogui) (10.0.0)\n",
      "Collecting pyperclip (from mouseinfo->pyautogui)\n",
      "  Using cached pyperclip-1.8.2-py3-none-any.whl\n",
      "Installing collected packages: pytweening, pyrect, pyperclip, pymsgbox, pyscreeze, pygetwindow, mouseinfo, pyautogui\n",
      "Successfully installed mouseinfo-0.1.3 pyautogui-0.9.54 pygetwindow-0.0.9 pymsgbox-1.0.9 pyperclip-1.8.2 pyrect-0.2.0 pyscreeze-0.1.30 pytweening-1.0.7\n"
     ]
    }
   ],
   "source": [
    "!pip install pyautogui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ef6cdb1-5084-4e0e-b084-5440c8cf2a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keyboard\n",
      "  Downloading keyboard-0.13.5-py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 0.0/58.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 58.1/58.1 kB 3.0 MB/s eta 0:00:00\n",
      "Installing collected packages: keyboard\n",
      "Successfully installed keyboard-0.13.5\n"
     ]
    }
   ],
   "source": [
    "!pip install keyboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed19d04-aed0-44c1-9940-cb67b54633e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyautogui\n",
    "import time\n",
    "\n",
    "while True:\n",
    "    pyautogui.keyDown('e')\n",
    "    time.sleep(1)\n",
    "    pyautogui.keyUp('e')\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56c60367-fe9d-4547-ab70-a2836870450c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import pyautogui\n",
    "\n",
    "\n",
    "# MediaPipe Hands initialization\n",
    "mphands = mp.solutions.hands\n",
    "hands = mphands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# GPU availability check\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Gesture recognition model path and class names\n",
    "model_path = \"./efficientnet2.pth\"\n",
    "classes_names = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T',\n",
    "                 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n",
    "img_size = 224\n",
    "model_name = \"efficientnet_b3a\"\n",
    "num_classes = len(classes_names)\n",
    "\n",
    "# Initialize the gesture recognition model\n",
    "class SELFMODEL(nn.Module):\n",
    "    def __init__(self, model_name, out_features=num_classes, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained)\n",
    "        if model_name[:3] == \"res\":\n",
    "            n_features = self.model.fc.in_features\n",
    "            self.model.fc = nn.Linear(n_features, out_features)\n",
    "        elif model_name[:3] == \"vit\":\n",
    "            n_features = self.model.head.in_features\n",
    "            self.model.head = nn.Linear(n_features, out_features)\n",
    "        else:\n",
    "            n_features = self.model.classifier.in_features\n",
    "            self.model.classifier = nn.Linear(n_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Function to get PyTorch transforms\n",
    "def get_torch_transforms(img_size=224):\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.RandomHorizontalFlip(p=0.2),\n",
    "            transforms.RandomRotation((-5, 5)),\n",
    "            transforms.RandomAutocontrast(p=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "    return data_transforms\n",
    "\n",
    "# Define a function to control the car based on hand gestures\n",
    "def gesture_control(recognized_gestures):\n",
    "    # Determine the corresponding car control action based on recognized gestures\n",
    "\n",
    "#    'B' or 'W' in recognized_gestures =  forward\n",
    "#   'N' in recognized_gestures =  Reverse\n",
    "#    'C' in recognized_gestures =  left\n",
    "#    'space' in recognized_gestures = right\n",
    "#    'A/S' in recognized_gestures = pause\n",
    "###\n",
    "    if 'B' in recognized_gestures:\n",
    "        pyautogui.keyDown('e')\n",
    "        time.sleep(1)\n",
    "        pyautogui.keyUp('e')\n",
    "        time.sleep(1)\n",
    "    elif 'O' in recognized_gestures:\n",
    "        pyautogui.keyDown('q')\n",
    "        time.sleep(1)\n",
    "        pyautogui.keyUp('q')\n",
    "        time.sleep(1)\n",
    "    elif 'N' in recognized_gestures:\n",
    "        pyautogui.keyDown('space')\n",
    "        time.sleep(1)\n",
    "        pyautogui.keyUp('space')\n",
    "        time.sleep(1)\n",
    "    else:\n",
    "        print(\"noting\")\n",
    "        # Apply the updated controls\n",
    "#    client.setCarControls(car_controls)\n",
    "\n",
    "\n",
    "# Initialize the gesture recognition model\n",
    "model = SELFMODEL(model_name=model_name, out_features=num_classes, pretrained=False)\n",
    "weights = torch.load(model_path, map_location=torch.device('cuda'))\n",
    "model.load_state_dict(weights)\n",
    "model.eval()\n",
    "\n",
    "# Initialize webcam capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Camera not found or cannot be opened.\")\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    # Capture a frame from the webcam\n",
    "    _, frame = cap.read()\n",
    "    h, w, c = frame.shape\n",
    "    if not _:\n",
    "        break\n",
    "\n",
    "    framergb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(framergb)\n",
    "    hand_landmarks = result.multi_hand_landmarks\n",
    "\n",
    "    if hand_landmarks:\n",
    "        for index, handLMs in enumerate(hand_landmarks):\n",
    "            x_max = 0\n",
    "            y_max = 0\n",
    "            x_min = w\n",
    "            y_min = h\n",
    "\n",
    "            for lm in handLMs.landmark:\n",
    "                x, y = int(lm.x * w), int(lm.y * h)\n",
    "                if x > x_max:\n",
    "                    x_max = x\n",
    "                if x < x_min:\n",
    "                    x_min = x\n",
    "                if y > y_max:\n",
    "                    y_max = y\n",
    "                if y < y_min:\n",
    "                    y_min = y\n",
    "\n",
    "            x_min = max(0, x_min - 50)\n",
    "            y_min = max(0, y_min - 50)\n",
    "            x_max = min(w, x_max + 50)\n",
    "            y_max = min(h, y_max + 50)\n",
    "\n",
    "            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "            cropped_frame = frame[y_min:y_max, x_min:x_max]\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "\n",
    "            input_tensor = transform(cropped_frame).unsqueeze(0)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(input_tensor)\n",
    "                probabilities = F.softmax(output[0], dim=0)\n",
    "                predicted_class = torch.argmax(probabilities).item()\n",
    "                # Call the gesture_control function to control the car based on recognized gestures\n",
    "\n",
    "\n",
    "            #use getsure control it\n",
    "                gesture_control(classes_names[predicted_class])\n",
    "            #end of it\n",
    "\n",
    "                predict_name = classes_names[predicted_class] + \" for Hand \" + str(index + 1)\n",
    "\n",
    "            cv2.putText(frame, predict_name, (10, 30 + 30 * index), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                        (255, 0 + index * 100, 0), 2)\n",
    "\n",
    "            # Draw hand landmarks on the cropped frame\n",
    "            mp_drawing.draw_landmarks(frame, handLMs, mphands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Display the cropped frame with landmarks\n",
    "            # cv2.imshow(\"Cropped Frame\", cropped_frame)\n",
    "\n",
    "    else:\n",
    "        cv2.putText(frame, \"nothing\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        #for brake the car\n",
    "\n",
    "        ###\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "        #\"esc\" to quit\n",
    "    if cv2.waitKey(100) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9204e53d-36bf-4b7a-b3ed-f177ece3793f",
   "metadata": {},
   "source": [
    "the second try  without the hand land mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3281c290-312d-4837-9079-6bdfdf0a1b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import keyboard\n",
    "\n",
    "\n",
    "# MediaPipe Hands initialization\n",
    "mphands = mp.solutions.hands\n",
    "hands = mphands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5)\n",
    "\n",
    "\n",
    "# GPU availability check\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Gesture recognition model path and class names\n",
    "model_path = \"./efficientnet2.pth\"\n",
    "classes_names = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T',\n",
    "                 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n",
    "img_size = 224\n",
    "model_name = \"efficientnet_b3a\"\n",
    "num_classes = len(classes_names)\n",
    "\n",
    "# Initialize the gesture recognition model\n",
    "class SELFMODEL(nn.Module):\n",
    "    def __init__(self, model_name, out_features=num_classes, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained)\n",
    "        if model_name[:3] == \"res\":\n",
    "            n_features = self.model.fc.in_features\n",
    "            self.model.fc = nn.Linear(n_features, out_features)\n",
    "        elif model_name[:3] == \"vit\":\n",
    "            n_features = self.model.head.in_features\n",
    "            self.model.head = nn.Linear(n_features, out_features)\n",
    "        else:\n",
    "            n_features = self.model.classifier.in_features\n",
    "            self.model.classifier = nn.Linear(n_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Function to get PyTorch transforms\n",
    "def get_torch_transforms(img_size=224):\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.RandomHorizontalFlip(p=0.2),\n",
    "            transforms.RandomRotation((-5, 5)),\n",
    "            transforms.RandomAutocontrast(p=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "    return data_transforms\n",
    "\n",
    "# Define a function to control the car based on hand gestures\n",
    "def gesture_control(recognized_gestures):\n",
    "    # Determine the corresponding car control action based on recognized gestures\n",
    "\n",
    "    if 'B' in recognized_gestures:\n",
    "        keyboard.press_and_release('e')\n",
    "    elif 'O' in recognized_gestures:\n",
    "        keyboard.press_and_release('q')\n",
    "    elif 'N' in recognized_gestures:\n",
    "        keyboard.press_and_release('space')\n",
    "    else:\n",
    "       \n",
    "\n",
    "# Initialize the gesture recognition model\n",
    "model = SELFMODEL(model_name=model_name, out_features=num_classes, pretrained=False)\n",
    "weights = torch.load(model_path, map_location=torch.device('cuda'))\n",
    "model.load_state_dict(weights)\n",
    "model.eval()\n",
    "\n",
    "# Initialize webcam capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Camera not found or cannot be opened.\")\n",
    "\n",
    "while True:\n",
    "    # Capture a frame from the webcam\n",
    "    _, frame = cap.read()\n",
    "    h, w, c = frame.shape\n",
    "    if not _:\n",
    "        break\n",
    "\n",
    "    framergb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(framergb)\n",
    "    hand_landmarks = result.multi_hand_landmarks\n",
    "\n",
    "    if hand_landmarks:\n",
    "        for index, handLMs in enumerate(hand_landmarks):\n",
    "            x_max = 0\n",
    "            y_max = 0\n",
    "            x_min = w\n",
    "            y_min = h\n",
    "\n",
    "            for lm in handLMs.landmark:\n",
    "                x, y = int(lm.x * w), int(lm.y * h)\n",
    "                if x > x_max:\n",
    "                    x_max = x\n",
    "                if x < x_min:\n",
    "                    x_min = x\n",
    "                if y > y_max:\n",
    "                    y_max = y\n",
    "                if y < y_min:\n",
    "                    y_min = y\n",
    "\n",
    "            x_min = max(0, x_min - 50)\n",
    "            y_min = max(0, y_min - 50)\n",
    "            x_max = min(w, x_max + 50)\n",
    "            y_max = min(h, y_max + 50)\n",
    "\n",
    "            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "            cropped_frame = frame[y_min:y_max, x_min:x_max]\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "\n",
    "            input_tensor = transform(cropped_frame).unsqueeze(0)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(input_tensor)\n",
    "                probabilities = F.softmax(output[0], dim=0)\n",
    "                predicted_class = torch.argmax(probabilities).item()\n",
    "                #use getsure control it\n",
    "                gesture_control(classes_names[predicted_class])\n",
    "                #end of it\n",
    "                predict_name = classes_names[predicted_class] + \" for Hand \" + str(index + 1)\n",
    "\n",
    "            cv2.putText(frame, predict_name, (10, 30 + 30 * index), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                        (255, 0 + index * 100, 0), 2)\n",
    "\n",
    "    else:\n",
    "        cv2.putText(frame, \"nothing\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "        #\"esc\" to quit\n",
    "    if cv2.waitKey(100) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d1a6c62-c922-46fe-8e2f-91672b9436b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import pyautogui\n",
    "\n",
    "\n",
    "# MediaPipe Hands initialization\n",
    "mphands = mp.solutions.hands\n",
    "hands = mphands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# GPU availability check\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Gesture recognition model path and class names\n",
    "model_path = \"./efficientnet2.pth\"\n",
    "classes_names = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T',\n",
    "                 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n",
    "img_size = 224\n",
    "model_name = \"efficientnet_b3a\"\n",
    "num_classes = len(classes_names)\n",
    "\n",
    "# Initialize the gesture recognition model\n",
    "class SELFMODEL(nn.Module):\n",
    "    def __init__(self, model_name, out_features=num_classes, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained)\n",
    "        if model_name[:3] == \"res\":\n",
    "            n_features = self.model.fc.in_features\n",
    "            self.model.fc = nn.Linear(n_features, out_features)\n",
    "        elif model_name[:3] == \"vit\":\n",
    "            n_features = self.model.head.in_features\n",
    "            self.model.head = nn.Linear(n_features, out_features)\n",
    "        else:\n",
    "            n_features = self.model.classifier.in_features\n",
    "            self.model.classifier = nn.Linear(n_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Function to get PyTorch transforms\n",
    "def get_torch_transforms(img_size=224):\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.RandomHorizontalFlip(p=0.2),\n",
    "            transforms.RandomRotation((-5, 5)),\n",
    "            transforms.RandomAutocontrast(p=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "    return data_transforms\n",
    "\n",
    "# Define a function to control the car based on hand gestures\n",
    "def gesture_control(recognized_gestures):\n",
    "    # Determine the corresponding car control action based on recognized gestures\n",
    "\n",
    "    if 'B' in recognized_gestures:\n",
    "        pyautogui.keyDown('e')\n",
    "        time.sleep(1)\n",
    "        pyautogui.keyUp('e')\n",
    "        time.sleep(1)\n",
    "    elif 'O' in recognized_gestures:\n",
    "        pyautogui.keyDown('q')\n",
    "        time.sleep(1)\n",
    "        pyautogui.keyUp('q')\n",
    "        time.sleep(1)\n",
    "    elif 'N' in recognized_gestures:\n",
    "        pyautogui.keyDown('space')\n",
    "        time.sleep(1)\n",
    "        pyautogui.keyUp('space')\n",
    "        time.sleep(1)\n",
    "    else:\n",
    "        print(\"noting\")\n",
    "        # Apply the updated controls\n",
    "#    client.setCarControls(car_controls)\n",
    "\n",
    "\n",
    "# Initialize the gesture recognition model\n",
    "model = SELFMODEL(model_name=model_name, out_features=num_classes, pretrained=False)\n",
    "weights = torch.load(model_path, map_location=torch.device('cuda'))\n",
    "model.load_state_dict(weights)\n",
    "model.eval()\n",
    "\n",
    "# Initialize webcam capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Camera not found or cannot be opened.\")\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    # Capture a frame from the webcam\n",
    "    _, frame = cap.read()\n",
    "    h, w, c = frame.shape\n",
    "    if not _:\n",
    "        break\n",
    "\n",
    "    framergb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(framergb)\n",
    "    hand_landmarks = result.multi_hand_landmarks\n",
    "\n",
    "    if hand_landmarks:\n",
    "        index = 0  # Initialize index\n",
    "\n",
    "        while index < len(hand_landmarks):\n",
    "            handLMs = hand_landmarks[index]\n",
    "\n",
    "            x_max = 0\n",
    "            y_max = 0\n",
    "            x_min = w\n",
    "            y_min = h\n",
    "\n",
    "            for lm in handLMs.landmark:\n",
    "                x, y = int(lm.x * w), int(lm.y * h)\n",
    "                if x > x_max:\n",
    "                    x_max = x\n",
    "                if x < x_min:\n",
    "                    x_min = x\n",
    "                if y > y_max:\n",
    "                    y_max = y\n",
    "                if y < y_min:\n",
    "                    y_min = y\n",
    "\n",
    "            x_min = max(0, x_min - 50)\n",
    "            y_min = max(0, y_min - 50)\n",
    "            x_max = min(w, x_max + 50)\n",
    "            y_max = min(h, y_max + 50)\n",
    "\n",
    "            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "            cropped_frame = frame[y_min:y_max, x_min:x_max]\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "\n",
    "            input_tensor = transform(cropped_frame).unsqueeze(0)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(input_tensor)\n",
    "                probabilities = F.softmax(output[0], dim=0)\n",
    "                predicted_class = torch.argmax(probabilities).item()\n",
    "\n",
    "            gesture_control(classes_names[predicted_class])\n",
    "\n",
    "            predict_name = classes_names[predicted_class] + \" for Hand \" + str(index + 1)\n",
    "\n",
    "            cv2.putText(frame, predict_name, (10, 30 + 30 * index), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                        (255, 0 + index * 100, 0), 2)\n",
    "\n",
    "            # Draw hand landmarks on the cropped frame\n",
    "            mp_drawing.draw_landmarks(frame, handLMs, mphands.HAND_CONNECTIONS)\n",
    "            index += 1  \n",
    "            # Increment index\n",
    "            # Display the cropped frame with landmarks\n",
    "            # cv2.imshow(\"Cropped Frame\", cropped_frame)\n",
    "\n",
    "    else:\n",
    "        cv2.putText(frame, \"nothing\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        #for brake the car\n",
    "\n",
    "        ###\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "        #\"esc\" to quit\n",
    "    if cv2.waitKey(100) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e282d5b4-35fa-4829-b719-8573b8d0af7b",
   "metadata": {},
   "source": [
    "## without the pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c094f15-bec6-43bf-9c69-b288492d5de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import pyautogui\n",
    "\n",
    "\n",
    "# MediaPipe Hands initialization\n",
    "mphands = mp.solutions.hands\n",
    "hands = mphands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5)\n",
    "\n",
    "\n",
    "# GPU availability check\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Gesture recognition model path and class names\n",
    "model_path = \"./efficientnet2.pth\"\n",
    "classes_names = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T',\n",
    "                 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n",
    "img_size = 224\n",
    "model_name = \"efficientnet_b3a\"\n",
    "num_classes = len(classes_names)\n",
    "\n",
    "# Initialize the gesture recognition model\n",
    "class SELFMODEL(nn.Module):\n",
    "    def __init__(self, model_name, out_features=num_classes, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained)\n",
    "        if model_name[:3] == \"res\":\n",
    "            n_features = self.model.fc.in_features\n",
    "            self.model.fc = nn.Linear(n_features, out_features)\n",
    "        elif model_name[:3] == \"vit\":\n",
    "            n_features = self.model.head.in_features\n",
    "            self.model.head = nn.Linear(n_features, out_features)\n",
    "        else:\n",
    "            n_features = self.model.classifier.in_features\n",
    "            self.model.classifier = nn.Linear(n_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define a function to control the car based on hand gestures\n",
    "def gesture_control(recognized_gestures):\n",
    "    # Determine the corresponding car control action based on recognized gestures\n",
    "\n",
    "    if 'B' in recognized_gestures:\n",
    "        pyautogui.keyDown('e')\n",
    "        time.sleep(1)\n",
    "        pyautogui.keyUp('e')\n",
    "        time.sleep(1)\n",
    "    elif 'O' in recognized_gestures:\n",
    "        pyautogui.keyDown('q')\n",
    "        time.sleep(1)\n",
    "        pyautogui.keyUp('q')\n",
    "        time.sleep(1)\n",
    "    elif 'N' in recognized_gestures:\n",
    "        pyautogui.keyDown('space')\n",
    "        time.sleep(1)\n",
    "        pyautogui.keyUp('space')\n",
    "        time.sleep(1)\n",
    "    else:\n",
    "        print(\"noting\")\n",
    "        # Apply the updated controls\n",
    "#    client.setCarControls(car_controls)\n",
    "\n",
    "\n",
    "# Initialize the gesture recognition model\n",
    "model = SELFMODEL(model_name=model_name, out_features=num_classes, pretrained=False)\n",
    "weights = torch.load(model_path, map_location=torch.device('cuda'))\n",
    "model.load_state_dict(weights)\n",
    "model.eval()\n",
    "\n",
    "# Initialize webcam capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Camera not found or cannot be opened.\")\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    # Capture a frame from the webcam\n",
    "    _, frame = cap.read()\n",
    "    h, w, c = frame.shape\n",
    "    if not _:\n",
    "        break\n",
    "\n",
    "    framergb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(framergb)\n",
    "    hand_landmarks = result.multi_hand_landmarks\n",
    "\n",
    "    if hand_landmarks:\n",
    "        index = 0  # Initialize index\n",
    "        while index < len(hand_landmarks):\n",
    "            handLMs = hand_landmarks[index]\n",
    "            x_max = 0\n",
    "            y_max = 0\n",
    "            x_min = w\n",
    "            y_min = h\n",
    "            for lm in handLMs.landmark:\n",
    "                x, y = int(lm.x * w), int(lm.y * h)\n",
    "                if x > x_max:\n",
    "                    x_max = x\n",
    "                if x < x_min:\n",
    "                    x_min = x\n",
    "                if y > y_max:\n",
    "                    y_max = y\n",
    "                if y < y_min:\n",
    "                    y_min = y\n",
    "            x_min = max(0, x_min - 50)\n",
    "            y_min = max(0, y_min - 50)\n",
    "            x_max = min(w, x_max + 50)\n",
    "            y_max = min(h, y_max + 50)\n",
    "\n",
    "            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "            cropped_frame = frame[y_min:y_max, x_min:x_max]\n",
    "\n",
    "            \n",
    "            transform = transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "\n",
    "            input_tensor = transform(cropped_frame).unsqueeze(0)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(input_tensor)\n",
    "                probabilities = F.softmax(output[0], dim=0)\n",
    "                predicted_class = torch.argmax(probabilities).item()\n",
    "\n",
    "            gesture_control(classes_names[predicted_class])\n",
    "\n",
    "            predict_name = classes_names[predicted_class] + \" for Hand \" + str(index + 1)\n",
    "\n",
    "            cv2.putText(frame, predict_name, (10, 30 + 30 * index), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                        (255, 0 + index * 100, 0), 2)\n",
    "\n",
    "            index += 1\n",
    "          \n",
    "\n",
    "    else:\n",
    "        cv2.putText(frame, \"nothing\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "       \n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "        #\"esc\" to quit\n",
    "    if cv2.waitKey(100) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af0acd9-22d7-4d1b-977e-4942947392d4",
   "metadata": {},
   "source": [
    "##  multithreading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a74ee4b-e9f9-4d12-bcf9-6bbe856787b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-13 (webcam_thread):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\p\\anaconda3\\Lib\\threading.py\", line 1038, in _bootstrap_inner\n",
      "Exception in thread Thread-14 (gesture_recognition_thread):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\p\\anaconda3\\Lib\\threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\p\\anaconda3\\Lib\\threading.py\", line 975, in run\n",
      "    self.run()\n",
      "  File \"C:\\Users\\p\\anaconda3\\Lib\\threading.py\", line 975, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "TypeError: webcam_thread() missing 2 required positional arguments: 'frame' and 'hand_landmarks'\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "TypeError: gesture_recognition_thread() missing 2 required positional arguments: 'frame' and 'hand_landmarks'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import pyautogui\n",
    "import threading\n",
    "\n",
    "# MediaPipe Hands initialization\n",
    "mphands = mp.solutions.hands\n",
    "hands = mphands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5)\n",
    "\n",
    "# GPU availability check\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Gesture recognition model path and class names\n",
    "model_path = \"./efficientnet2.pth\"\n",
    "classes_names = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T',\n",
    "                 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n",
    "img_size = 224\n",
    "model_name = \"efficientnet_b3a\"\n",
    "num_classes = len(classes_names)\n",
    "\n",
    "#############################\n",
    "stop_threads = False\n",
    "\n",
    "cropped_frame = None\n",
    "\n",
    "\n",
    "#############################\n",
    "\n",
    "\n",
    "# Define a flag to indicate when to stop the threads\n",
    "\n",
    "class SELFMODEL(nn.Module):\n",
    "    def __init__(self, model_name, out_features=num_classes, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained)\n",
    "        if model_name[:3] == \"res\":\n",
    "            n_features = self.model.fc.in_features\n",
    "            self.model.fc = nn.Linear(n_features, out_features)\n",
    "        elif model_name[:3] == \"vit\":\n",
    "            n_features = self.model.head.in_features\n",
    "            self.model.head = nn.Linear(n_features, out_features)\n",
    "        else:\n",
    "            n_features = self.model.classifier.in_features\n",
    "            self.model.classifier = nn.Linear(n_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define a function to control the car based on hand gestures\n",
    "def gesture_control(recognized_gestures):\n",
    "    # Determine the corresponding car control action based on recognized gestures\n",
    "\n",
    "    if 'B' in recognized_gestures:\n",
    "        pyautogui.keyDown('e')\n",
    "        time.sleep(1)\n",
    "        pyautogui.keyUp('e')\n",
    "        time.sleep(1)\n",
    "    elif 'O' in recognized_gestures:\n",
    "        pyautogui.keyDown('q')\n",
    "        time.sleep(1)\n",
    "        pyautogui.keyUp('q')\n",
    "        time.sleep(1)\n",
    "    elif 'N' in recognized_gestures:\n",
    "        pyautogui.keyDown('space')\n",
    "        time.sleep(1)\n",
    "        pyautogui.keyUp('space')\n",
    "        time.sleep(1)\n",
    "    else:\n",
    "        print(\"noting\")\n",
    "\n",
    "\n",
    "#########################################################\n",
    "# Function to capture webcam frames and perform hand gesture recognition\n",
    "def webcam_thread(frame, hand_landmarks):\n",
    "    global stop_threads\n",
    "    global shared_cropped_frame\n",
    "    stop_threads = False\n",
    "    while not stop_threads:\n",
    "\n",
    "        if hand_landmarks:\n",
    "            index = 0  # Initialize index\n",
    "            while index < len(hand_landmarks):\n",
    "                handLMs = hand_landmarks[index]\n",
    "                x_max = 0\n",
    "                y_max = 0\n",
    "                x_min = w\n",
    "                y_min = h\n",
    "                for lm in handLMs.landmark:\n",
    "                    x, y = int(lm.x * w), int(lm.y * h)\n",
    "                    if x > x_max:\n",
    "                        x_max = x\n",
    "                    if x < x_min:\n",
    "                        x_min = x\n",
    "                    if y > y_max:\n",
    "                        y_max = y\n",
    "                    if y < y_min:\n",
    "                        y_min = y\n",
    "                x_min = max(0, x_min - 50)\n",
    "                y_min = max(0, y_min - 50)\n",
    "                x_max = min(w, x_max + 50)\n",
    "                y_max = min(h, y_max + 50)\n",
    "                cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "            stop_threads = True\n",
    "        else:\n",
    "            cv2.putText(frame, \"nothing\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            cv2.imshow(\"Frame\", frame)\n",
    "            stop_threads = True\n",
    "    return frame\n",
    "\n",
    "\n",
    "# Function to perform gesture recognition\n",
    "def gesture_recognition_thread(frame, hand_landmarks):\n",
    "    global stop_threads\n",
    "    stop_threads = False\n",
    "    while not stop_threads:\n",
    "\n",
    "        if hand_landmarks:\n",
    "            index = 0  # Initialize index\n",
    "            while index < len(hand_landmarks):\n",
    "                handLMs = hand_landmarks[index]\n",
    "                x_max = 0\n",
    "                y_max = 0\n",
    "                x_min = w\n",
    "                y_min = h\n",
    "                for lm in handLMs.landmark:\n",
    "                    x, y = int(lm.x * w), int(lm.y * h)\n",
    "                    if x > x_max:\n",
    "                        x_max = x\n",
    "                    if x < x_min:\n",
    "                        x_min = x\n",
    "                    if y > y_max:\n",
    "                        y_max = y\n",
    "                    if y < y_min:\n",
    "                        y_min = y\n",
    "                x_min = max(0, x_min - 50)\n",
    "                y_min = max(0, y_min - 50)\n",
    "                x_max = min(w, x_max + 50)\n",
    "                y_max = min(h, y_max + 50)\n",
    "\n",
    "                cropped_frame = frame[y_min:y_max, x_min:x_max]\n",
    "                transform = transforms.Compose([\n",
    "                    transforms.ToPILImage(),\n",
    "                    transforms.Resize((224, 224)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                ])\n",
    "                input_tensor = transform(cropped_frame).unsqueeze(0)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    output = model(input_tensor)\n",
    "                    probabilities = F.softmax(output[0], dim=0)\n",
    "                    predicted_class = torch.argmax(probabilities).item()\n",
    "\n",
    "                gesture_control(classes_names[predicted_class])\n",
    "                predict_name = classes_names[predicted_class] + \" for Hand \" + str(index + 1)\n",
    "                cv2.putText(frame, predict_name, (10, 30 + 30 * index), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                            (255, 0 + index * 100, 0), 2)\n",
    "                index += 1\n",
    "            stop_threads = True\n",
    "        else:\n",
    "            cv2.putText(frame, \"nothing\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            stop_threads = True\n",
    "    return frame\n",
    "\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "# Initialize the gesture recognition model\n",
    "model = SELFMODEL(model_name=model_name, out_features=num_classes, pretrained=False)\n",
    "weights = torch.load(model_path, map_location=torch.device('cuda'))\n",
    "model.load_state_dict(weights)\n",
    "model.eval()\n",
    "\n",
    "# Initialize webcam capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Camera not found or cannot be opened.\")\n",
    "\n",
    "#######################################\n",
    "# Start the webcam capture thread\n",
    "webcam_thread = threading.Thread(target=webcam_thread)\n",
    "webcam_thread.start()\n",
    "\n",
    "# Start the gesture recognition thread\n",
    "gesture_thread = threading.Thread(target=gesture_recognition_thread)\n",
    "gesture_thread.start()\n",
    "\n",
    "#####################################\n",
    "frame = cv2\n",
    "hand_landmarks\n",
    "\n",
    "######################################\n",
    "# Main loop\n",
    "while True:\n",
    "\n",
    "    global frame, hand_landmarks\n",
    "\n",
    "    # Capture a frame from the webcam\n",
    "    _, frame = cap.read()\n",
    "    if not _:\n",
    "        print(\"Error: Failed to capture frame\")\n",
    "        break\n",
    "    h, w, c = frame.shape\n",
    "\n",
    "    framergb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(framergb)\n",
    "    hand_landmarks = result.multi_hand_landmarks\n",
    "\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "    # Terminate the threads when 'esc' key is pressed\n",
    "    if cv2.waitKey(100) & 0xFF == 27:\n",
    "        stop_threads = True\n",
    "        break\n",
    "\n",
    "# Wait for threads to finish before releasing resources\n",
    "webcam_thread.join()\n",
    "gesture_thread.join()\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "##########################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68196206-ba47-4479-a11c-c9f7c3d9cf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import pyautogui\n",
    "import threading\n",
    "\n",
    "# MediaPipe Hands initialization\n",
    "mphands = mp.solutions.hands\n",
    "hands = mphands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5)\n",
    "\n",
    "# GPU availability check\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Gesture recognition model path and class names\n",
    "model_path = \"./efficientnet2.pth\"\n",
    "classes_names = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T',\n",
    "                 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n",
    "img_size = 224\n",
    "model_name = \"efficientnet_b3a\"\n",
    "num_classes = len(classes_names)\n",
    "\n",
    "#############################\n",
    "stop_threads = False\n",
    "\n",
    "cropped_frame = None\n",
    "\n",
    "\n",
    "#############################\n",
    "\n",
    "\n",
    "# Define a flag to indicate when to stop the threads\n",
    "\n",
    "class SELFMODEL(nn.Module):\n",
    "    def __init__(self, model_name, out_features=num_classes, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained)\n",
    "        if model_name[:3] == \"res\":\n",
    "            n_features = self.model.fc.in_features\n",
    "            self.model.fc = nn.Linear(n_features, out_features)\n",
    "        elif model_name[:3] == \"vit\":\n",
    "            n_features = self.model.head.in_features\n",
    "            self.model.head = nn.Linear(n_features, out_features)\n",
    "        else:\n",
    "            n_features = self.model.classifier.in_features\n",
    "            self.model.classifier = nn.Linear(n_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define a function to control the car based on hand gestures\n",
    "def gesture_control(recognized_gestures):\n",
    "    # Determine the corresponding car control action based on recognized gestures\n",
    "\n",
    "    if 'B' in recognized_gestures:\n",
    "        pyautogui.keyDown('e')\n",
    "        time.sleep(1)\n",
    "        pyautogui.keyUp('e')\n",
    "        time.sleep(1)\n",
    "    elif 'O' in recognized_gestures:\n",
    "        pyautogui.keyDown('q')\n",
    "        time.sleep(1)\n",
    "        pyautogui.keyUp('q')\n",
    "        time.sleep(1)\n",
    "    elif 'N' in recognized_gestures:\n",
    "        pyautogui.keyDown('space')\n",
    "        time.sleep(1)\n",
    "        pyautogui.keyUp('space')\n",
    "        time.sleep(1)\n",
    "    else:\n",
    "        print(\"noting\")\n",
    "\n",
    "\n",
    "#########################################################\n",
    "# Function to capture webcam frames and perform hand gesture recognition\n",
    "class WebcamThread(threading.Thread):\n",
    "    def __init__(self, cap, hands):\n",
    "        super(WebcamThread, self).__init__()\n",
    "        self.cap = cap\n",
    "        self.hands = hands\n",
    "        self.stop_threads = False\n",
    "\n",
    "    def run(self):\n",
    "        while not self.stop_threads:\n",
    "            _, frame = self.cap.read()\n",
    "            if not _:\n",
    "                print(\"Error: Failed to capture frame\")\n",
    "                break\n",
    "\n",
    "            h, w, c = frame.shape\n",
    "\n",
    "            framergb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            result = self.hands.process(framergb)\n",
    "            hand_landmarks = result.multi_hand_landmarks\n",
    "\n",
    "            if hand_landmarks:\n",
    "                index = 0  # Initialize index\n",
    "                while index < len(hand_landmarks):\n",
    "                    # ... (your existing logic for drawing rectangles)\n",
    "                    index += 1\n",
    "            else:\n",
    "                cv2.putText(frame, \"nothing\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "        self.cap.release()\n",
    "\n",
    "# Function to perform gesture recognition\n",
    "class GestureRecognitionThread(threading.Thread):\n",
    "    def __init__(self, frame, hands, model, class_names):\n",
    "        super(GestureRecognitionThread, self).__init__()\n",
    "        self.frame = frame\n",
    "        self.hands = hands\n",
    "        self.model = model\n",
    "        self.class_names = class_names\n",
    "        self.stop_threads = False\n",
    "\n",
    "    def run(self):\n",
    "        while not self.stop_threads:\n",
    "            _, frame = self.frame.read()\n",
    "            if not _:\n",
    "                print(\"Error: Failed to capture frame\")\n",
    "                break\n",
    "\n",
    "            h, w, c = frame.shape\n",
    "\n",
    "            framergb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            result = self.hands.process(framergb)\n",
    "            hand_landmarks = result.multi_hand_landmarks\n",
    "\n",
    "            if hand_landmarks:\n",
    "                index = 0  # Initialize index\n",
    "                while index < len(hand_landmarks):\n",
    "                    # ... (your existing logic for gesture recognition)\n",
    "                    index += 1\n",
    "            else:\n",
    "                cv2.putText(frame, \"nothing\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "# Initialize the gesture recognition model\n",
    "model = SELFMODEL(model_name=model_name, out_features=num_classes, pretrained=False)\n",
    "weights = torch.load(model_path, map_location=torch.device('cuda'))\n",
    "model.load_state_dict(weights)\n",
    "model.eval()\n",
    "\n",
    "# Initialize webcam capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Camera not found or cannot be opened.\")\n",
    "\n",
    "#######################################\n",
    "\n",
    "# Initialize the gesture recognition model\n",
    "model = SELFMODEL(model_name=model_name, out_features=num_classes, pretrained=False)\n",
    "weights = torch.load(model_path, map_location=torch.device('cuda'))\n",
    "model.load_state_dict(weights)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# Create instances of the thread classes\n",
    "webcam_thread = WebcamThread(cap, hands)\n",
    "gesture_thread = GestureRecognitionThread(cap, hands, model, classes_names)\n",
    "\n",
    "# Start the threads\n",
    "webcam_thread.start()\n",
    "gesture_thread.start()\n",
    "\n",
    "#####################################\n",
    "\n",
    "######################################\n",
    "# Main loop\n",
    "while True:\n",
    "\n",
    "    global frame, hand_landmarks\n",
    "\n",
    "    # Capture a frame from the webcam\n",
    "    _, frame = cap.read()\n",
    "    if not _:\n",
    "        print(\"Error: Failed to capture frame\")\n",
    "        break\n",
    "    h, w, c = frame.shape\n",
    "\n",
    "    framergb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(framergb)\n",
    "    hand_landmarks = result.multi_hand_landmarks\n",
    "\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "    # Terminate the threads when 'esc' key is pressed\n",
    "    if cv2.waitKey(100) & 0xFF == 27:\n",
    "        stop_threads = True\n",
    "        break\n",
    "\n",
    "# Wait for threads to finish before releasing resources\n",
    "webcam_thread.join()\n",
    "gesture_thread.join()\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "##########################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc457291-9d08-45d7-8f53-0cfddfb10459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import pyautogui\n",
    "\n",
    "\n",
    "\n",
    "mphands = mp.solutions.hands\n",
    "hands = mphands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5)\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "\n",
    "model_path = \"./efficientnet2.pth\"\n",
    "classes_names = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T',\n",
    "                 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n",
    "img_size = 224\n",
    "model_name = \"efficientnet_b3a\"\n",
    "num_classes = len(classes_names)\n",
    "\n",
    "\n",
    "class SELFMODEL(nn.Module):\n",
    "    def __init__(self, model_name, out_features=num_classes, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained)\n",
    "        if model_name[:3] == \"res\":\n",
    "            n_features = self.model.fc.in_features\n",
    "            self.model.fc = nn.Linear(n_features, out_features)\n",
    "        elif model_name[:3] == \"vit\":\n",
    "            n_features = self.model.head.in_features\n",
    "            self.model.head = nn.Linear(n_features, out_features)\n",
    "        else:\n",
    "            n_features = self.model.classifier.in_features\n",
    "            self.model.classifier = nn.Linear(n_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "def gesture_control(recognized_gestures):\n",
    "    if 'B' in recognized_gestures:\n",
    "        pyautogui.keyDown('e')\n",
    "        pyautogui.keyUp('e')\n",
    "    elif 'O' in recognized_gestures:\n",
    "        pyautogui.keyDown('q')\n",
    "        pyautogui.keyUp('q')\n",
    "    elif 'N' in recognized_gestures:\n",
    "        pyautogui.keyDown('space')\n",
    "        pyautogui.keyUp('space')\n",
    "    else:\n",
    "        print(\"noting\")\n",
    "\n",
    "\n",
    "def find_bounding_box(landmarks, w, h):\n",
    "    x_min, y_min, x_max, y_max = float('inf'), float('inf'), float('-inf'), float('-inf')\n",
    "    for lm in landmarks:\n",
    "        x, y = int(lm.x * w), int(lm.y * h)\n",
    "        x_min, y_min = min(x_min, x), min(y_min, y)\n",
    "        x_max, y_max = max(x_max, x), max(y_max, y)\n",
    "    # Add a margin of 50 pixels to the bounding box\n",
    "    x_min, y_min = max(0, x_min - 50), max(0, y_min - 50)\n",
    "    x_max, y_max = min(w, x_max + 50), min(h, y_max + 50)\n",
    "    return x_min, y_min, x_max, y_max\n",
    "\n",
    "def transform_frame(frame):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    return transform(frame).unsqueeze(0)\n",
    "\n",
    "def classify_gesture(model, input_tensor, classes_names):\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        probabilities = F.softmax(output[0], dim=0)\n",
    "        predicted_class = torch.argmax(probabilities).item()\n",
    "    return predicted_class\n",
    "\n",
    "def display_gesture_prediction(frame, gesture_name, index):\n",
    "    cv2.putText(frame, f\"{gesture_name} for Hand {index + 1}\", (10, 30 + 30 * index), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                (255, 0 + index * 100, 0), 2)\n",
    "\n",
    "\n",
    "model = SELFMODEL(model_name=model_name, out_features=num_classes, pretrained=False)\n",
    "weights = torch.load(model_path, map_location=torch.device('cuda'))\n",
    "model.load_state_dict(weights)\n",
    "model.eval()\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Camera not found or cannot be opened.\")\n",
    "index = 0\n",
    "\n",
    "# Downsample factor (skip every N frames)\n",
    "downsample_factor = 1\n",
    "downsample_counter = 0\n",
    "classA=\"nothing\"\n",
    "\n",
    "while True:\n",
    "    \n",
    "    _, frame = cap.read()\n",
    "    h, w, c = frame.shape\n",
    "    if not _:\n",
    "        break\n",
    "    # Downsample frames\n",
    "    if downsample_counter % downsample_factor == 0:\n",
    "        framergb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        result = hands.process(framergb)\n",
    "        hand_landmarks = result.multi_hand_landmarks\n",
    "        if hand_landmarks:\n",
    "            index = 0\n",
    "            while index < len(hand_landmarks):\n",
    "                handLMs = hand_landmarks[index]\n",
    "                x_min, y_min, x_max, y_max = find_bounding_box(handLMs.landmark, w, h)\n",
    "                cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "                cropped_frame = frame[y_min:y_max, x_min:x_max]\n",
    "                input_tensor = transform_frame(cropped_frame)\n",
    "                predicted_class = classify_gesture(model, input_tensor, classes_names)\n",
    "                gesture_control(classes_names[predicted_class])\n",
    "                classA= classes_names[predicted_class]\n",
    "                display_gesture_prediction(frame, classes_names[predicted_class], index)\n",
    "                index += 1\n",
    "        else:\n",
    "            cv2.putText(frame, \"nothing\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            break\n",
    "    else:\n",
    "            gesture_control(classA)\n",
    "\n",
    "    downsample_counter += 1\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bcb9cbe-d817-4170-b5f5-3811ef3ac4fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 110\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    109\u001b[0m     _, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m--> 110\u001b[0m     h, w, c \u001b[38;5;241m=\u001b[39m \u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _:\n\u001b[0;32m    112\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import pyautogui\n",
    "\n",
    "\n",
    "mphands = mp.solutions.hands\n",
    "hands = mphands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "\n",
    "model_path = \"./efficientnet_b20240123.pth\"\n",
    "#classes_names = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T','U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n",
    "classes_names = ['A', 'B', 'Blank', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "img_size = 224\n",
    "model_name = \"efficientnet_b3a\"\n",
    "num_classes = len(classes_names)\n",
    "\n",
    "\n",
    "class SELFMODEL(nn.Module):\n",
    "    def __init__(self, model_name, out_features=num_classes, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained)\n",
    "        if model_name[:3] == \"res\":\n",
    "            n_features = self.model.fc.in_features\n",
    "            self.model.fc = nn.Linear(n_features, out_features)\n",
    "        elif model_name[:3] == \"vit\":\n",
    "            n_features = self.model.head.in_features\n",
    "            self.model.head = nn.Linear(n_features, out_features)\n",
    "        else:\n",
    "            n_features = self.model.classifier.in_features\n",
    "            self.model.classifier = nn.Linear(n_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "def gesture_control(recognized_gestures):\n",
    "    if 'B' in recognized_gestures:\n",
    "        pyautogui.keyDown('e')\n",
    "        pyautogui.keyUp('e')\n",
    "    elif 'O' in recognized_gestures:\n",
    "        pyautogui.keyDown('q')\n",
    "        pyautogui.keyUp('q')\n",
    "    elif 'N' in recognized_gestures:\n",
    "        pyautogui.keyDown('space')\n",
    "        pyautogui.keyUp('space')\n",
    "    else:\n",
    "        print(\"noting\")\n",
    "\n",
    "\n",
    "def find_bounding_box(landmarks, w, h):\n",
    "    x_min, y_min, x_max, y_max = float('inf'), float('inf'), float('-inf'), float('-inf')\n",
    "    for lm in landmarks:\n",
    "        x, y = int(lm.x * w), int(lm.y * h)\n",
    "        x_min, y_min = min(x_min, x), min(y_min, y)\n",
    "        x_max, y_max = max(x_max, x), max(y_max, y)\n",
    "    # Add a margin of 50 pixels to the bounding box\n",
    "    x_min, y_min = max(0, x_min - 50), max(0, y_min - 50)\n",
    "    x_max, y_max = min(w, x_max + 50), min(h, y_max + 50)\n",
    "    return x_min, y_min, x_max, y_max\n",
    "\n",
    "def transform_frame(frame):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    return transform(frame).unsqueeze(0)\n",
    "\n",
    "def classify_gesture(model, input_tensor, classes_names):\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        probabilities = F.softmax(output[0], dim=0)\n",
    "        predicted_class = torch.argmax(probabilities).item()\n",
    "    return predicted_class\n",
    "\n",
    "def display_gesture_prediction(frame, gesture_name, index):\n",
    "    cv2.putText(frame, f\"{gesture_name} for Hand {index + 1}\", (10, 30 + 30 * index), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                (255, 0 + index * 100, 0), 2)\n",
    "\n",
    "\n",
    "model = SELFMODEL(model_name=model_name, out_features=num_classes, pretrained=False)\n",
    "weights = torch.load(model_path, map_location=torch.device('cuda'))\n",
    "model.load_state_dict(weights)\n",
    "model.eval()\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Camera not found or cannot be opened.\")\n",
    "index = 0\n",
    "\n",
    "# Downsample factor (skip every N frames)\n",
    "downsample_factor = 1\n",
    "downsample_counter = 0\n",
    "classA=\"nothing\"\n",
    "\n",
    "while True:\n",
    "\n",
    "    _, frame = cap.read()\n",
    "    h, w, c = frame.shape\n",
    "    if not _:\n",
    "        break\n",
    "    # Downsample frames\n",
    "    if downsample_counter % downsample_factor == 0:\n",
    "        framergb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        result = hands.process(framergb)\n",
    "        hand_landmarks = result.multi_hand_landmarks\n",
    "        if hand_landmarks:\n",
    "            index = 0\n",
    "            while index < len(hand_landmarks):\n",
    "                handLMs = hand_landmarks[index]\n",
    "                x_min, y_min, x_max, y_max = find_bounding_box(handLMs.landmark, w, h)\n",
    "                cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "                cropped_frame = frame[y_min:y_max, x_min:x_max]\n",
    "                input_tensor = transform_frame(cropped_frame)\n",
    "                predicted_class = classify_gesture(model, input_tensor, classes_names)\n",
    "                gesture_control(classes_names[predicted_class])\n",
    "                classA= classes_names[predicted_class]\n",
    "                display_gesture_prediction(frame, classes_names[predicted_class], index)\n",
    "                index += 1\n",
    "        else:\n",
    "            cv2.putText(frame, \"nothing\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "       \n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            break\n",
    "    else:\n",
    "            gesture_control(classA)\n",
    "\n",
    "    downsample_counter += 1\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98371de-70ea-4995-a88f-c43174c36c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import pyautogui\n",
    "\n",
    "\n",
    "\n",
    "mphands = mp.solutions.hands\n",
    "hands = mphands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5)\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "\n",
    "model_path = \"./efficientnet2.pth\"\n",
    "classes_names = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T',\n",
    "                 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n",
    "img_size = 224\n",
    "model_name = \"efficientnet_b3a\"\n",
    "num_classes = len(classes_names)\n",
    "\n",
    "\n",
    "class SELFMODEL(nn.Module):\n",
    "    def __init__(self, model_name, out_features=num_classes, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained)\n",
    "        if model_name[:3] == \"res\":\n",
    "            n_features = self.model.fc.in_features\n",
    "            self.model.fc = nn.Linear(n_features, out_features)\n",
    "        elif model_name[:3] == \"vit\":\n",
    "            n_features = self.model.head.in_features\n",
    "            self.model.head = nn.Linear(n_features, out_features)\n",
    "        else:\n",
    "            n_features = self.model.classifier.in_features\n",
    "            self.model.classifier = nn.Linear(n_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "def gesture_control(recognized_gestures):\n",
    "    if 'B' in recognized_gestures:\n",
    "        pyautogui.keyDown('e')\n",
    "        pyautogui.keyUp('e')\n",
    "    elif 'O' in recognized_gestures:\n",
    "        pyautogui.keyDown('q')\n",
    "        pyautogui.keyUp('q')\n",
    "    elif 'N' in recognized_gestures:\n",
    "        pyautogui.keyDown('space')\n",
    "        pyautogui.keyUp('space')\n",
    "    else:\n",
    "        print(\"noting\")\n",
    "\n",
    "\n",
    "def find_bounding_box(landmarks, w, h):\n",
    "    x_min, y_min, x_max, y_max = float('inf'), float('inf'), float('-inf'), float('-inf')\n",
    "    for lm in landmarks:\n",
    "        x, y = int(lm.x * w), int(lm.y * h)\n",
    "        x_min, y_min = min(x_min, x), min(y_min, y)\n",
    "        x_max, y_max = max(x_max, x), max(y_max, y)\n",
    "    # Add a margin of 50 pixels to the bounding box\n",
    "    x_min, y_min = max(0, x_min - 50), max(0, y_min - 50)\n",
    "    x_max, y_max = min(w, x_max + 50), min(h, y_max + 50)\n",
    "    return x_min, y_min, x_max, y_max\n",
    "\n",
    "def transform_frame(frame):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    return transform(frame).unsqueeze(0)\n",
    "\n",
    "def classify_gesture(model, input_tensor, classes_names):\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        probabilities = F.softmax(output[0], dim=0)\n",
    "        predicted_class = torch.argmax(probabilities).item()\n",
    "    return predicted_class\n",
    "\n",
    "def display_gesture_prediction(frame, gesture_name, index):\n",
    "    cv2.putText(frame, f\"{gesture_name} for Hand {index + 1}\", (10, 30 + 30 * index), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                (255, 0 + index * 100, 0), 2)\n",
    "\n",
    "\n",
    "model = SELFMODEL(model_name=model_name, out_features=num_classes, pretrained=False)\n",
    "weights = torch.load(model_path, map_location=torch.device('cuda'))\n",
    "model.load_state_dict(weights)\n",
    "model.eval()\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Camera not found or cannot be opened.\")\n",
    "index = 0\n",
    "\n",
    "# Downsample factor (skip every N frames)\n",
    "downsample_factor = 1\n",
    "downsample_counter = 0\n",
    "classA=\"nothing\"\n",
    "\n",
    "while True:\n",
    "\n",
    "    _, frame = cap.read()\n",
    "    h, w, c = frame.shape\n",
    "    if not _:\n",
    "        break\n",
    "    # Downsample frames\n",
    "    if downsample_counter % downsample_factor == 0:\n",
    "        framergb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        result = hands.process(framergb)\n",
    "        hand_landmarks = result.multi_hand_landmarks\n",
    "        if hand_landmarks:\n",
    "            index = 0\n",
    "            while index < len(hand_landmarks):\n",
    "                handLMs = hand_landmarks[index]\n",
    "                x_min, y_min, x_max, y_max = find_bounding_box(handLMs.landmark, w, h)\n",
    "                cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "                cropped_frame = frame[y_min:y_max, x_min:x_max]\n",
    "                input_tensor = transform_frame(cropped_frame)\n",
    "                predicted_class = classify_gesture(model, input_tensor, classes_names)\n",
    "                gesture_control(classes_names[predicted_class])\n",
    "                classA= classes_names[predicted_class]\n",
    "                display_gesture_prediction(frame, classes_names[predicted_class], index)\n",
    "                index += 1\n",
    "        else:\n",
    "            cv2.putText(frame, \"nothing\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            break\n",
    "    else:\n",
    "            gesture_control(classA)\n",
    "\n",
    "    downsample_counter += 1\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d13b4613-5d9c-46f9-8cbf-aac29f913b59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n",
      "noting\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import pyautogui\n",
    "\n",
    "mphands = mp.solutions.hands\n",
    "hands = mphands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "model_path = \"./efficientnet_b20240123.pth\"\n",
    "classes_names = ['A', 'B', 'Blank', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S',\n",
    "                 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "img_size = 224\n",
    "model_name = \"efficientnet_b3a\"\n",
    "num_classes = len(classes_names)\n",
    "\n",
    "\n",
    "class SELFMODEL(nn.Module):\n",
    "    def __init__(self, model_name, out_features=num_classes, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained)\n",
    "        if model_name[:3] == \"res\":\n",
    "            n_features = self.model.fc.in_features\n",
    "            self.model.fc = nn.Linear(n_features, out_features)\n",
    "        elif model_name[:3] == \"vit\":\n",
    "            n_features = self.model.head.in_features\n",
    "            self.model.head = nn.Linear(n_features, out_features)\n",
    "        else:\n",
    "            n_features = self.model.classifier.in_features\n",
    "            self.model.classifier = nn.Linear(n_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def gesture_control(recognized_gestures):\n",
    "    if 'B' in recognized_gestures:\n",
    "        pyautogui.keyDown('e')\n",
    "        pyautogui.keyUp('e')\n",
    "    elif 'S' in recognized_gestures:\n",
    "        pyautogui.keyDown('q')\n",
    "        pyautogui.keyUp('q')\n",
    "    elif 'O' in recognized_gestures:\n",
    "        pyautogui.keyDown('space')\n",
    "        pyautogui.keyUp('space')\n",
    "    else:\n",
    "        print(\"noting\")\n",
    "\n",
    "\n",
    "def find_bounding_box(landmarks, w, h):\n",
    "    x_min, y_min, x_max, y_max = float('inf'), float('inf'), float('-inf'), float('-inf')\n",
    "    for lm in landmarks:\n",
    "        x, y = int(lm.x * w), int(lm.y * h)\n",
    "        x_min, y_min = min(x_min, x), min(y_min, y)\n",
    "        x_max, y_max = max(x_max, x), max(y_max, y)\n",
    "    # Add a margin of 50 pixels to the bounding box\n",
    "    x_min, y_min = max(0, x_min - 50), max(0, y_min - 50)\n",
    "    x_max, y_max = min(w, x_max + 50), min(h, y_max + 50)\n",
    "    return x_min, y_min, x_max, y_max\n",
    "\n",
    "\n",
    "def transform_frame(frame):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    return transform(frame).unsqueeze(0)\n",
    "\n",
    "\n",
    "def classify_gesture(model, input_tensor, classes_names):\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        probabilities = F.softmax(output[0], dim=0)\n",
    "        predicted_class = torch.argmax(probabilities).item()\n",
    "    return predicted_class\n",
    "\n",
    "\n",
    "def display_gesture_prediction(frame, gesture_name, index):\n",
    "    cv2.putText(frame, f\"{gesture_name} for Hand {index + 1}\", (10, 30 + 30 * index), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                (255, 0 + index * 100, 0), 2)\n",
    "\n",
    "\n",
    "model = SELFMODEL(model_name=model_name, out_features=num_classes, pretrained=False)\n",
    "weights = torch.load(model_path, map_location=torch.device('cuda'))\n",
    "model.load_state_dict(weights)\n",
    "model.eval()\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Camera not found or cannot be opened.\")\n",
    "index = 0\n",
    "\n",
    "# Downsample factor (skip every N frames)\n",
    "downsample_factor = 3\n",
    "downsample_counter = 0\n",
    "\n",
    "while True:\n",
    "\n",
    "    _, frame = cap.read()\n",
    "    h, w, c = frame.shape\n",
    "    if not _:\n",
    "        break\n",
    "    # Downsample frames\n",
    "    if downsample_counter % downsample_factor == 0:\n",
    "        framergb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        result = hands.process(framergb)\n",
    "        hand_landmarks = result.multi_hand_landmarks\n",
    "        if hand_landmarks:\n",
    "            index = 0\n",
    "            while index < len(hand_landmarks):\n",
    "                handLMs = hand_landmarks[index]\n",
    "                x_min, y_min, x_max, y_max = find_bounding_box(handLMs.landmark, w, h)\n",
    "                cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "                cropped_frame = frame[y_min:y_max, x_min:x_max]\n",
    "                input_tensor = transform_frame(cropped_frame)\n",
    "                predicted_class = classify_gesture(model, input_tensor, classes_names)\n",
    "                gesture_control(classes_names[predicted_class])\n",
    "                classA = classes_names[predicted_class]\n",
    "                display_gesture_prediction(frame, classes_names[predicted_class], index)\n",
    "                index += 1\n",
    "        else:\n",
    "            cv2.putText(frame, \"nothing\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "    downsample_counter += 1\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
