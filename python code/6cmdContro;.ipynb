{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a9b9c9-c3ff-44c1-ac5d-e9317f6b4cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import threading\n",
    "import queue\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import torchvision.transforms as transforms\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "\n",
    "mphands = mp.solutions.hands\n",
    "hands = mphands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "model_path = \"./efficientnet1.pth\"\n",
    "classes_names = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T',\n",
    "                 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n",
    "img_size = 224\n",
    "model_name = \"efficientnet_b3a\"\n",
    "num_classes = len(classes_names)\n",
    "\n",
    "\n",
    "class SELFMODEL(nn.Module):\n",
    "    def __init__(self, model_name, out_features=num_classes, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained)\n",
    "        if model_name[:3] == \"res\":\n",
    "            n_features = self.model.fc.in_features\n",
    "            self.model.fc = nn.Linear(n_features, out_features)\n",
    "        elif model_name[:3] == \"vit\":\n",
    "            n_features = self.model.head.in_features\n",
    "            self.model.head = nn.Linear(n_features, out_features)\n",
    "        else:\n",
    "            n_features = self.model.classifier.in_features\n",
    "            self.model.classifier = nn.Linear(n_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define the gesture recognition function\n",
    "def gesture_control(recognized_gestures):\n",
    "    if 'space' in recognized_gestures:\n",
    "        pyautogui.keyDown('e')\n",
    "        pyautogui.keyUp('e')\n",
    "    elif 'O' in recognized_gestures:\n",
    "        pyautogui.keyDown('q')\n",
    "        pyautogui.keyUp('q')\n",
    "    elif 'N' in recognized_gestures:\n",
    "        pyautogui.keyDown('w')\n",
    "        pyautogui.keyUp('w')\n",
    "    elif 'B' in recognized_gestures:\n",
    "        pyautogui.keyDown('s')\n",
    "        pyautogui.keyUp('s')\n",
    "    elif 'S' in recognized_gestures:\n",
    "        pyautogui.keyDown('a')\n",
    "        pyautogui.keyUp('a')\n",
    "    elif 'F' in recognized_gestures:\n",
    "        pyautogui.keyDown('d')\n",
    "        pyautogui.keyUp('d')\n",
    "\n",
    "\n",
    "# Define the frame processing function\n",
    "def process_frames():\n",
    "    while True:\n",
    "        frame = frame_queue.get()\n",
    "        h, w, c = frame.shape\n",
    "        framergb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        result = hands.process(framergb)\n",
    "        hand_landmarks = result.multi_hand_landmarks\n",
    "        if hand_landmarks:\n",
    "            index = 0\n",
    "            while index < len(hand_landmarks):\n",
    "                handLMs = hand_landmarks[index]\n",
    "                x_min, y_min, x_max, y_max = find_bounding_box(handLMs.landmark, w, h)\n",
    "                cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "                cropped_frame = frame[y_min:y_max, x_min:x_max]\n",
    "                input_tensor = transform_frame(cropped_frame)\n",
    "                predicted_class = classify_gesture(model, input_tensor, classes_names)\n",
    "                gesture_control(classes_names[predicted_class])\n",
    "                display_gesture_prediction(frame, classes_names[predicted_class], index)\n",
    "                index += 1\n",
    "        else:\n",
    "            cv2.putText(frame, \"nothing\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "\n",
    "# Define the helper functions (find_bounding_box, transform_frame, classify_gesture, display_gesture_prediction)\n",
    "\n",
    "\n",
    "def find_bounding_box(landmarks, w, h):\n",
    "    x_min, y_min, x_max, y_max = float('inf'), float('inf'), float('-inf'), float('-inf')\n",
    "    for lm in landmarks:\n",
    "        x, y = int(lm.x * w), int(lm.y * h)\n",
    "        x_min, y_min = min(x_min, x), min(y_min, y)\n",
    "        x_max, y_max = max(x_max, x), max(y_max, y)\n",
    "    # Add a margin of 50 pixels to the bounding box\n",
    "    x_min, y_min = max(0, x_min - 50), max(0, y_min - 50)\n",
    "    x_max, y_max = min(w, x_max + 50), min(h, y_max + 50)\n",
    "    return x_min, y_min, x_max, y_max\n",
    "\n",
    "\n",
    "def transform_frame(frame):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    return transform(frame).unsqueeze(0)\n",
    "\n",
    "\n",
    "def classify_gesture(model, input_tensor, classes_names):\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        probabilities = F.softmax(output[0], dim=0)\n",
    "        predicted_class = torch.argmax(probabilities).item()\n",
    "    return predicted_class\n",
    "\n",
    "\n",
    "def display_gesture_prediction(frame, gesture_name, index):\n",
    "    cv2.putText(frame, f\"{gesture_name} for Hand {index + 1}\", (10, 30 + 30 * index), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                (255, 0 + index * 100, 0), 2)\n",
    "\n",
    "\n",
    "model = SELFMODEL(model_name=model_name, out_features=num_classes, pretrained=False)\n",
    "weights = torch.load(model_path, map_location=torch.device('cuda'))\n",
    "model.load_state_dict(weights)\n",
    "model.eval()\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Camera not found or cannot be opened.\")\n",
    "index = 0\n",
    "\n",
    "# Create a queue for passing frames between capture and processing threads\n",
    "frame_queue = queue.Queue(maxsize=5)\n",
    "\n",
    "# Create video capture object\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "# Start capture thread\n",
    "def capture_frames():\n",
    "    while True:\n",
    "        _, frame = cap.read()\n",
    "        if not _:\n",
    "            break\n",
    "        frame_queue.put(frame)\n",
    "\n",
    "\n",
    "# Start processing thread\n",
    "processing_thread = threading.Thread(target=process_frames)\n",
    "processing_thread.start()\n",
    "\n",
    "# Start capture thread\n",
    "capture_thread = threading.Thread(target=capture_frames)\n",
    "capture_thread.start()\n",
    "\n",
    "# Wait for threads to finish\n",
    "capture_thread.join()\n",
    "processing_thread.join()\n",
    "\n",
    "# Release video capture object and destroy windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32a09b3c-8b83-4b97-8054-4591df50c9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import pyautogui\n",
    "\n",
    "\n",
    "\n",
    "mphands = mp.solutions.hands\n",
    "hands = mphands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5)\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "\n",
    "model_path = \"./efficientnet1.pth\"\n",
    "classes_names = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T',\n",
    "                 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n",
    "img_size = 224\n",
    "model_name = \"efficientnet_b3a\"\n",
    "num_classes = len(classes_names)\n",
    "\n",
    "\n",
    "class SELFMODEL(nn.Module):\n",
    "    def __init__(self, model_name, out_features=num_classes, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained)\n",
    "        if model_name[:3] == \"res\":\n",
    "            n_features = self.model.fc.in_features\n",
    "            self.model.fc = nn.Linear(n_features, out_features)\n",
    "        elif model_name[:3] == \"vit\":\n",
    "            n_features = self.model.head.in_features\n",
    "            self.model.head = nn.Linear(n_features, out_features)\n",
    "        else:\n",
    "            n_features = self.model.classifier.in_features\n",
    "            self.model.classifier = nn.Linear(n_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "def gesture_control(recognized_gestures):\n",
    "    if 'space' in recognized_gestures:\n",
    "        pyautogui.keyDown('e')\n",
    "        pyautogui.keyUp('e')\n",
    "    elif 'O' in recognized_gestures:\n",
    "        pyautogui.keyDown('q')\n",
    "        pyautogui.keyUp('q')\n",
    "    elif 'N' in recognized_gestures:\n",
    "        pyautogui.keyDown('w')\n",
    "        pyautogui.keyUp('w')\n",
    "    elif 'B' in recognized_gestures:\n",
    "        pyautogui.keyDown('s')\n",
    "        pyautogui.keyUp('s')\n",
    "    elif 'S' in recognized_gestures:\n",
    "        pyautogui.keyDown('a')\n",
    "        pyautogui.keyUp('a')\n",
    "    elif 'F' in recognized_gestures:\n",
    "        pyautogui.keyDown('d')\n",
    "        pyautogui.keyUp('d')\n",
    "    \n",
    "        #do nothing\n",
    "\n",
    "\n",
    "def find_bounding_box(landmarks, w, h):\n",
    "    x_min, y_min, x_max, y_max = float('inf'), float('inf'), float('-inf'), float('-inf')\n",
    "    for lm in landmarks:\n",
    "        x, y = int(lm.x * w), int(lm.y * h)\n",
    "        x_min, y_min = min(x_min, x), min(y_min, y)\n",
    "        x_max, y_max = max(x_max, x), max(y_max, y)\n",
    "    # Add a margin of 50 pixels to the bounding box\n",
    "    x_min, y_min = max(0, x_min - 50), max(0, y_min - 50)\n",
    "    x_max, y_max = min(w, x_max + 50), min(h, y_max + 50)\n",
    "    return x_min, y_min, x_max, y_max\n",
    "\n",
    "def transform_frame(frame):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    return transform(frame).unsqueeze(0)\n",
    "\n",
    "def classify_gesture(model, input_tensor, classes_names):\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        probabilities = F.softmax(output[0], dim=0)\n",
    "        predicted_class = torch.argmax(probabilities).item()\n",
    "    return predicted_class\n",
    "\n",
    "def display_gesture_prediction(frame, gesture_name, index):\n",
    "    cv2.putText(frame, f\"{gesture_name} for Hand {index + 1}\", (10, 30 + 30 * index), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                (255, 0 + index * 100, 0), 2)\n",
    "\n",
    "\n",
    "model = SELFMODEL(model_name=model_name, out_features=num_classes, pretrained=False)\n",
    "weights = torch.load(model_path, map_location=torch.device('cuda'))\n",
    "model.load_state_dict(weights)\n",
    "model.eval()\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Camera not found or cannot be opened.\")\n",
    "index = 0\n",
    "\n",
    "# Downsample factor (skip every N frames)\n",
    "downsample_factor = 1\n",
    "downsample_counter = 0\n",
    "classA=\"nothing\"\n",
    "\n",
    "while True:\n",
    "\n",
    "    _, frame = cap.read()\n",
    "    h, w, c = frame.shape\n",
    "    if not _:\n",
    "        break\n",
    "    # Downsample frames\n",
    "    if downsample_counter % downsample_factor == 0:\n",
    "        framergb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        result = hands.process(framergb)\n",
    "        hand_landmarks = result.multi_hand_landmarks\n",
    "        if hand_landmarks:\n",
    "            index = 0\n",
    "            while index < len(hand_landmarks):\n",
    "                handLMs = hand_landmarks[index]\n",
    "                x_min, y_min, x_max, y_max = find_bounding_box(handLMs.landmark, w, h)\n",
    "                cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "                cropped_frame = frame[y_min:y_max, x_min:x_max]\n",
    "                input_tensor = transform_frame(cropped_frame)\n",
    "                predicted_class = classify_gesture(model, input_tensor, classes_names)\n",
    "                gesture_control(classes_names[predicted_class])\n",
    "                classA= classes_names[predicted_class]\n",
    "                display_gesture_prediction(frame, classes_names[predicted_class], index)\n",
    "                index += 1\n",
    "        else:\n",
    "            cv2.putText(frame, \"nothing\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            break\n",
    "    else:\n",
    "            gesture_control(classA)\n",
    "\n",
    "    downsample_counter += 1\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
